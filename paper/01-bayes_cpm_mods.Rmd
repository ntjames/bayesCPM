
<!--
### Paper ###

1a) start by writing up Bayes ordinal CPM for continuous outcome (i.e. extensions to prior spec to deal with large # of intercepts). 

-> Use model with dirichlet and several concentration params
for n # unique obs/categories
alpha=1/n
alpha=0? (NO! stan_polr and ord_mod1 don't work w/ improper alpha=0 conc. param)
alpha=1/2 (multinomial Jeffrey's prior)
alpha=1 ('uniform' prior)
alpha=1/(0.8 + 0.35 max(n, 3)) (conc param from Frank's sims)

Appendix showing derivation of posterior, what prior will be equivalent to MLE

other possible priors
-> truncated normal (McKinley 2015)
-> parameterization on differences in intercepts or log difference in intercepts?
e.g. alternate parameterization delta_1=log(gamma_1); delta_j=log(gamma_j-gamma_{j-1}) from Albert & Chib 1997, Fahrmeir & Tutz 1994, Ishwaran 2000 or just gamma_j-gamma_{j-1}
-> Congdon 2005 describes truncated normal and reparameterized

1c) simulations comparing Bayes CPM to NPMLE? - need to verify models are similar, especially in case where likelihood not well approximated by quadratic function

1d) simulations showing posterior predictive checks? Bayesian p-value or other Bayesian model checking

1e) Case study with HIV data. Add model checking steps

TO DO
- Clean up section describing basic Bayesian ordinal model

- Write section on extension to continuous, 'proof' related to priors for intercepts/heuristic argument?

lower priority 
lower priority -> 1b) figure out why hardcoded stan model doesn't match stan_polr when alpha=1, 0.5, 0.2, 2, 5, 1/ncat (other vals?), 
-> maybe change hardcoded stan model to est. cutpoints when all x are at their means (rather than all x=0 which I think is in implementation as of 10/10/19)
-> could also be related to parameterization of betas in stan_polr

### R package ###

TO DO

- add formula interface
- note about centering predictors OR automatic centering of predictors (see brms and rstanarm which both do this)
- clean up & update CDF, Mean, Quantile to accept list like rms::contrast rather than newdata data.frame to facilitate comparisons?
- figure out Stan issue with non-symmetric link function 
- add vignette using all examples from orm() documentation
- verify priors are correct (Jacobian for induced dirichlet? See Betancourt ordinal regression blog post) - DONE, don't need Jacobian if using pw_log_lik function as in stan_polr, also see  STAN User's guide 2.18.0 - 16.2 Reparameterizations and 16.3
--use roxygen function notation similar to 'Copula modeling with R' - DONE

- code for basic simulations
-- extend to more iterations or other scenarios (see Yuqi and Qi papers?) 
-- make sure model is working well in basic scenario; compare to NPMLE
using Bias, coverage, other statistics?

lower priority -> implement CPM models in OpenBUGS??
-- see http://www.omori.e.u-tokyo.ac.jp/WinBUGS/index.htm ex 14, 15, maybe 16
-- see https://gist.github.com/bartneck/038fae866e10641b8121679cd02db52e
-- see http://www.openbugs.net/Examples/Inhalers.html

-->


<!--
### Extensions/Future papers ###

2a) Use Dirichlet process prior and compare Bayes CPM to Bayesian nonparametric (BNP) model

-> implement BNP model for comparison to basic BayesCPM (JAGS? OpenBUGS? dirichletprocess package?, other?)
-- see https://cran.r-project.org/web/packages/dirichletprocess/index.html
https://cran.r-project.org/web/packages/BNSP/index.html
https://www.stats.ox.ac.uk/~teh/research/npbayes/Teh2010a.pdf


2b) Non-proportional odds. extend to weighted PPO vs. NPO type model (McKinley), except using continuous data and mixture links. [from Frank] consider adding a summarization of evidence for the candidate link functions, e.g., posterior probability that the model has a logit link.  And develop ways to summarize exposure/treatment effects in these mixtures of models, e.g. covariate-specific estimates of both odds ratios and risk differences, plus perhaps hazard ratio.

2c) mixture link model (maybe, depends on whether it's truly needed or improves model)
--finite mixture and BNP mixture over, e.g. normals
-- see brms mixture() and BDA3

2d) compare to single-index models?

2e) Extend to nonlinear model - try something simple like Emax
-- see https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html
-- see http://mc-stan.org/rstanarm/articles/glmer.html
-- see https://mc-stan.org/rstanarm/reference/stan_nlmer.html

lower priority -> code for nonlinear model
-- see stan_nlmer and vignette
-- see vignette("brms_nonlinear")
-- https://discourse.mc-stan.org/t/hierarchical-nonlinear-regression/4382/12
-- https://mattstats.wordpress.com/2017/06/12/fitting-nonlinear-functions-in-stan/


2f) Extend to mixed effects CPMs

-> code for mixed effects model
-- see Sorensen et al 2016 paper and STAN for linear mixed models at https://people.bath.ac.uk/jjf23/stan/

2g) nonlinear mixed effects model for Bayesian PK/PD?

other lower priority issues/future extensions:

* repeated measures/longitudinal with PPO vs. NPO -- with a eye to applying these models for repeated measures ordinal data with censoring and a terminating event (e.g. ischemia QOL & death). The first steps for this part are determining what sort of monotonic censoring patterns/restrictions can be handled and how to correctly account for variance of estimates. Exploration of the relationship between this model formulation and a multistate model with censoring and an absorbing state could be relevant  <-- might need to move this to another paper??

* repeated ordinal w/ terminating event and censoring

* multiple detection limits within or across sites (hierarch on groups defined by detection limit)

-->


<!-- ## Background -->

<!-- mini lit review for ordinal models and summarizing results of previous continuous CPM papers -->

Cumulative models for ordinal outcomes -- traditionally denoted cumulative link models (Agresti book citation) -- have been discussed extensively in the literature using both classical (frequentist) and Bayesian implementations. As these models are characterized by adding probabilities, not link functions, we prefer the nomenclature cumulative probability model (CPM). Under the frequentist paradigm, Walker and Duncan [@walker_estimation_1967] and McCullagh [@peter_mccullagh_regression_1980] described these models as an extension of dichotomous outcome regression models such as logistic and probit regression. A Bayesian CPM for ordinal regression was first detailed by Albert and Chib [@albert_bayesian_1993; @albert_bayesian_1997] and Johnson and Albert [@johnson_ordinal_1999]. Additional Bayesian CPMs including partial proportional odds[peterson and harrell], mixture link models [Lang], location-scale ordinal regression [citation] and multivariate ordinal outcomes are described in Congdon [@congdon_bayesian_2005]. In all these settings, the number of ordered outcome categories is implicitly assumed to be smaller (often much smaller) than the sample size. However, continuous data are also ordinal and can therefore be fit using CPMs. 

Harrell [@harrell_regression_2015] and Liu [@liu_modeling_2017] describe how non-parametric maximum likelihood (NPMLE) estimation can be used for continuous outcomes under the CPM modeling framework. A key insight for these models is that continuous data can also be viewed as ordinal categorical data. CPMs for continuous outcomes have several advantages including invariance of regression coefficient estimates to monotonic outcome transformations and ability to handle mixed continuous/discrete outcomes such as those that arise with a lower or upper limits of detection. In addition CPMs directly model the full conditional cumulative distribution function, which also allows estimates of means, quantiles, and exceedance probabilities to be calculated from a single model.


<!-- other somewhat similar Bayesian models?? -->
Mallick [???] - Similar model in context of survival data, also incorporates BNP prior, others??

<!-- DESCRIBE PROBLEMS/GAPS in literature using previous methods here and how we solve them

- contrasts and other functionals?
- incorporate prior info?
- similar strengths to Bayesian Survival - check there for addt'l benefits

-->

<!-- what did we do? --> 
In this paper, we develop Bayesian CPMs for continuous and mixed outcomes. Bayesian CPMs inherit many of the properties of CPMs estimated using NPMLE and also have additional advantages including interpretation using posterior probabilities, the ability to incorporate available prior information, and exact inference (within simulation error) <!--when the log-likelihood is not well-approximated by a quadratic function [!! need to show example where this is the case in simulations]-->
for quantities of interest without resorting to asymptotic approximations.
A primary challenge for the implementation of Bayesian CPMs for continuous outcomes is the specification of priors for the intercept parameters and we describe several proposed strategies. Through simulations, we explore characteristics of Bayesian CPMs using several model specifications and prior combinations <!--and make comparisons to CPM estimates using NPMLE and Bayesian nonparametric (BNP) regression, another popular approach for flexibly modeling outcomes-->. Finally, we present a case study involving HIV biomarker data for which outcomes are both skewed and censored at a lower limit of detection.

# Methods

## Cumulative Probability Model formulation
<!-- see Albert & Chib 1997 - sec 2, and Johnson & Albert Ordinal Data Modeling to clean up this section-->
Let $Y_i$ be the outcome for individual $i=1,\ldots,n$ with $p$ associated covariates $\boldsymbol{X_i}=(X_{i1},\ldots,X_{ip})$ such that each $Y_i$ falls into one of $j=1,\ldots, J$ ordered categories. The $Y_i$ can be modeled using a $Categorical(\boldsymbol{\pi_i})$ or $Multinomial(1,\boldsymbol{\pi_i})$ distribution where $\boldsymbol{\pi_i}=(\pi_{i1}, \ldots, \pi_{iJ})$ are the probabilities of individual $i$ being in category $j$ and $\sum_{j=1}^{J}=1$. Note that the value of the $\pi_{ij}$ are dependent on $x_i$, but we suppress the conditional notation except where it is needed for clarity. The cumulative probability of falling into category $j$ or a lower category is $Pr(Y_i \le j)=\eta_{ij}=\sum_{k=1}^{j}\pi_{ik}$. The CPM relates the cumulative probabilities to the observed covariates through a monotonically increasing link function $G^{-1}(\eta_{ij})=\gamma_{j}-\boldsymbol{x_i'\beta}$ or equivalently
\begin{gather}
Pr(Y \le j|X,\beta,\gamma)=\eta_{ij}=G(\gamma_{j}-\boldsymbol{x_i'\beta})
\end{gather}
Where the $\gamma_j$ are latent continuous cutpoints $-\infty \equiv \gamma_0 < \gamma_1 < \cdots < \gamma_{J-1} <\gamma_J \equiv \infty$ and $\boldsymbol{\beta}$ is a vector of $p$ coefficients.
For identifiability, the linear predictor $\boldsymbol{x_i'\beta}$ does not include an intercept, $\beta_0$. Common choices for $G^{-1}(\cdot)$ are logit, $G^{-1}(p)=\log\left(\frac{p}{1-p}\right)$; probit, $G^{-1}(p)=\Phi^{-1}(p)$; and complementary log-log, $G^{-1}(p)=\log(-\log(1-p))$. The probabilities of category membership are 
\begin{gather}
\label{eq:cellprobs}
\pi_{ij}=\eta_{i,j}-\eta_{i,j-1}=G(\gamma_j-\boldsymbol{x_i'\beta})-G(\gamma_{j-1}-\boldsymbol{x_i'\beta})
\end{gather}
Where the function $G(\cdot)$ is a cdf defined as the inverse of the link function (e.g., standard logistic and standard normal for the logit and probit links, respectively). The likelihood for an iid sample of observations $(y_1,\ldots,y_n)$ is
\begin{gather}
p(\boldsymbol{y}|\boldsymbol{x},\boldsymbol{\gamma},\boldsymbol{\beta})=
\prod_{j=1}^{J}\prod_{i:y_i=j}[G(\gamma_j-\boldsymbol{x_i'\beta})-G(\gamma_{j-1}-\boldsymbol{x_i'\beta})]
\end{gather}
For continuous data with no ties $J=n$. Letting $r(y_i)$ be the rank of $y_i$ the likelihood reduces to
\begin{gather}
p(\boldsymbol{y}|\boldsymbol{x},\boldsymbol{\gamma},\boldsymbol{\beta})=
\prod_{i=1}^{n}[G(\gamma_{r(y_i)}-\boldsymbol{x_i'\beta})-G(\gamma_{r(y_i)-1}-\boldsymbol{x_i'\beta})]
\end{gather}

<!--
As noted by Johnson & Albert [@johnson_ordinal_1999], the model with $J$ categories and $J-1$ cutpoints is nonidentifiable. There are two approaches to resolve this issue. The first approach fixes the value of one cutpoint or equivalently fixes the intercept in the regression function $Z=\boldsymbol{x'\beta} + \epsilon$ to be constant. Applying noninformative priors for the remaining cutpoints and $\beta$s along with the constraint $\gamma_1 \le \cdots \le \gamma_{J-1}$ results in maximum a posteriori (MAP) estimates which are equivalent to MLE estimates if there are moderate counts observed in each category. 

A second approach defines a probability distribution on the vector of cutpoints. Rather than specifying a prior directly on the $\gamma_i$, it is more intuitive to specify priors on the probability of falling into category $j$ for a fixed $\boldsymbol{x_i}$ and then use the specified distribution function to transform to the correct scale. If the observed $\boldsymbol{y}=(y_1, \ldots, y_n)$ are assumed independent given $\boldsymbol{\pi_i}=(\pi_{i1},\ldots,\pi_{iJ})$ the likelihood of $\boldsymbol{y}$ is proportional to a multinomial density $p(\boldsymbol{y}|\boldsymbol{\pi_i}) \propto \prod_{i=1}^{n} \pi_{ij}$. Using the prior $p(\boldsymbol{\gamma},\boldsymbol{\beta})$, the joint posterior distribution for ($\boldsymbol{\gamma},\boldsymbol{\beta})$ is
\begin{gather}
p(\boldsymbol{\gamma},\boldsymbol{\beta}|\boldsymbol{x},\boldsymbol{y}) \propto
p(\boldsymbol{\gamma},\boldsymbol{\beta})\prod_{i=1}^{n} \pi_{ij}=
p(\boldsymbol{\gamma},\boldsymbol{\beta})\prod_{i=1}^{n}[F(\gamma_{y_i}-\boldsymbol{x_i}'\beta )-F(\gamma_{y_i-1}-\boldsymbol{x_i}'\beta)]
\end{gather} 
!!! in above need summation/prod over $j$??, or indicator that for obs $i$ it is in category $j$ !!!
-->

To complete the model specification we define priors for the parameters $p(\boldsymbol{\beta},\boldsymbol{\gamma})$. We assume *a priori* independence between $\beta$ and $\gamma$ so $p(\boldsymbol{\beta},\boldsymbol{\gamma})=p(\boldsymbol{\beta})p(\boldsymbol{\gamma})$. To simplify the model formulation we also assume noninformative priors for the regression coefficients, $p(\boldsymbol{\beta}) \propto \boldsymbol{1}$ however weakly informative or informative priors can also be used. <!--conditional means approach for defining joint prior fro Johnson & ALbert sec 4.2.2 pp 132 -->

Specifying priors for $\boldsymbol{\gamma}$ is more challenging because of the ordering restriction. Several approaches have been suggested in the traditional CPM setting where $J \ll n$. McKinley et al. [@mckinley_bayesian_2015] and Congdon [@congdon_bayesian_2005] describe a sequentially truncated prior distribution that takes the form $p(\boldsymbol{\gamma})=p(\gamma_1)\prod_{j=2}^{J-1}p(\gamma_i|\gamma_{j-1})$ where the support of $p(\gamma_1)$ is $\mathbb{R}$ and the support of $p(\gamma_j)$ for $j=2,\ldots, J-1$ is $(\gamma_{j-1},\infty)$. For example using normal and truncated normal priors, $p(\gamma_1)\sim N(0, \sigma_\gamma^2)$ and $\gamma_j|\gamma_{j-1} \sim N(0, \sigma_\gamma^2)I(\gamma_{j-1},\infty)$. A second approach described by Albert and Chib [@albert_bayesian_1997] defines the prior on a transformation of the cutpoints to an unconstrained space. Let $\delta_1=\log(\gamma_1)$ and $\delta_j=\log(\gamma_j - \gamma_{j-1}),\, 2 \le j \le J-1$. Then a multivariate prior can be assigned to $\boldsymbol{\delta}$, e.g. $\boldsymbol{\delta} \sim N_{J-1}(\boldsymbol{\mu_0},\boldsymbol{\Sigma_0})$. 
Both approaches provide priors that satisfy the ordering restriction for $\boldsymbol{\gamma}$, but may be cumbersome when the number of unique continuous data values is high.

<!--
First approach requires recursive dependence of priors, need to tune variance then sample from sequential series of truncated distributions
The second approach requires specification of the $J-1$ $\mu_0$ vector and the $J-1 \times J-1$ covariance matrix.

Specifying priors for $\boldsymbol{\gamma}$ presents a challenge because of the ordering restriction and high dimensionality.
 noninformative has undesirable properties with low cell counts, see Johnson & Albert 4.2.1
 how sensitive are approaches above to sample size & cell counts?
-->

We instead adopt a third approach which defines a prior on $\boldsymbol{\pi_i}$ for a prespecified covariate vector and utilizes the transformation defined by $G(\cdot)$ to induce a prior on the $\boldsymbol{\gamma}$ [@betancourt_ordinal_2019].
Let $\pi_{.j} \equiv Pr(r(y)=j|x=0)$ be the probability of being in category $j$ when all the covariates are 0 (or at their mean values if covariates have been centered) and $\boldsymbol{\pi_{.}}=(\pi_{.1},\ldots,\pi_{.J})$. From equation (\ref{eq:cellprobs}) it follows that
\begin{gather}
\pi_{.j}=G(\gamma_j-0)-G(\gamma_{j-1}-0)=G(\gamma_j)-G(\gamma_{j-1})
\end{gather}
These equations define a transformation $h(\boldsymbol{\gamma})=\boldsymbol{\pi_{.}}$ between the cutpoints and probabilities of category membership when $\boldsymbol{X}=\boldsymbol{0}$. Conversely,
\begin{gather}
\sum_{k=1}^{j}\pi_{.k}=\sum_{k=1}^{j}\left[G(\gamma_k)-G(\gamma_{k-1})\right]=G(\gamma_j)
\end{gather}
so $G^{-1}\left(\sum_{k=1}^{j}\pi_{.k}\right)=\gamma_j$ defines the inverse transformation. 

Because $\boldsymbol{y}$ has a multinomial distribution a conjugate Dirichlet distribution is a natural choice of prior for $\boldsymbol{\pi_{.}}$. Setting $p(\boldsymbol{\pi_{.}}|\boldsymbol{\alpha}) \propto \prod_{j=1}^{J}\pi_{.j}^{\alpha_j-1}$ the posterior distribution is
\begin{align}
p(\boldsymbol{\gamma},\boldsymbol{\beta}|\boldsymbol{x},\boldsymbol{y}) & \propto p(\boldsymbol{\gamma})p(\boldsymbol{\beta}) p(\boldsymbol{y}|\boldsymbol{x},\boldsymbol{\gamma},\boldsymbol{\beta})\\
&\propto p(h(\boldsymbol{\gamma}))|\mathcal{J}|p(\boldsymbol{\beta}) p(\boldsymbol{y}|\boldsymbol{x},\boldsymbol{\gamma},\boldsymbol{\beta})\\
\label{eq:post}
&\propto p(\boldsymbol{\pi_{\cdot}}|\boldsymbol{\alpha})|\mathcal{J}|p(\boldsymbol{\beta}) p(\boldsymbol{y}|\boldsymbol{x},\boldsymbol{\gamma},\boldsymbol{\beta})
\end{align}
where $\mathcal{J}$ is the Jacobian of the transformation $h(\boldsymbol{\gamma})=\boldsymbol{\pi_{.}}$. Letting $\Omega=\sum_{j=1}^J\pi_{.j}=1$ be the constraint that the category probabilites must sum to 1, the entries in $\mathcal{J}$ where $\mathcal{J}_{r,c}$ is the term in row $r$ and column $c$ are
\begin{align*}
\mathcal{J}_{j,1}&=\frac{\partial \pi_{.j}}{\partial \Omega}=1 \quad
\mathcal{J}_{j+1,j+1}&=\frac{\partial \pi_{.j+1}}{\partial \gamma_j}=\frac{\partial}{\partial \gamma_j}\left[G(\gamma_{j+1})-G(\gamma_{j})\right]=-g(\gamma_j) \quad
\mathcal{J}_{j,j+1}&=\frac{\partial \pi_{.j}}{\partial \gamma_j}=\frac{\partial}{\partial \gamma_j}\left[G(\gamma_{j})-G(\gamma_{j-1})\right]=g(\gamma_j)
\end{align*}
Where $j=1,\ldots,J-1$, $g(\cdot)$ is the pdf of the distribution $G(\cdot)$, and $\mathcal{J}_{r,c}=0$ for all other entries so the form of the Jacobian is

<!--
 \begin{vmatrix} 
 \mathcal{J}_{11} & \mathcal{J}_{12} & \mathcal{J}_{13} & \mathcal{J}_{14}\\
 \mathcal{J}_{21} & \mathcal{J}_{22} & \mathcal{J}_{23} & \mathcal{J}_{24}\\
 \mathcal{J}_{31} & \mathcal{J}_{32} & \mathcal{J}_{33} & \mathcal{J}_{34}\\
 \mathcal{J}_{41} & \mathcal{J}_{42} & \mathcal{J}_{43} & \mathcal{J}_{44}\\
 \end{vmatrix}\\
 = 
-->
\begin{equation}
\begin{vmatrix} 
1      &  g(\gamma_1)  &  0            & 0           & \cdots           & 0 \\
1      & -g(\gamma_1)  &  g(\gamma_2)  & 0           & \cdots           & 0 \\
1      & 0             & -g(\gamma_2)  & g(\gamma_3) & \cdots           & 0 \\
\vdots & \vdots        &  \vdots       & \vdots      & \ddots           & \vdots \\
1      & 0             &  0            & 0           & -g(\gamma_{j-1}) & g(\gamma_j)\\
1      & 0             &  0            & 0           & 0                & -g(\gamma_j)\\
\end{vmatrix}
\end{equation}

While it is possible to define separate $\alpha_j$ parameters, we restrict our attention to symmetric Dirichlet distributions which use a single $\alpha$ value for all categories (i.e., $\alpha_1 = \alpha_2 = \cdots = \alpha_J$). The symmetric Dirichlet prior on $\boldsymbol{\pi_{.}}$ along with the transformation $h(\cdot)$ induces a prior for the $\boldsymbol{\gamma}$ with $\boldsymbol{\alpha}$ controlling the concentration of the induced prior. Figure \@ref(fig:probit-induced) shows induced $\boldsymbol{\gamma}$ priors assuming a probit link for several possible choices of concentration parameter and number of categories. The choices correspond to several options for a multinomial-Dirichlet model [@gelman_bayesian_2014] <!-- sec 3.4 -->: a uniform Dirichlet ($\boldsymbol{\alpha}=1$), Jeffrey's prior ($\boldsymbol{\alpha}=1/2$), an overall objective prior recommended by Berger et al. [@berger_overall_2015] ($\boldsymbol{\alpha}=1/J$), and an additional empirical determined reciprocal formula ($\boldsymbol{\alpha}=1/(0.8+0.35J)$).

In multiparameter models, the optimal choice of reference prior depends on the parameter or statistic of interest (e.g. $\beta$, conditional CDF, conditional mean) [@berger_overall_2015].  Without prior information, we seek a value of $\alpha$ that will have a minimal impact on inference for a variety of settings and quantities of interest while still producing posterior estimates that can be well sampled by the MCMC algorithm.

The model in (\ref{eq:post}) is implemented using the `R` interface to Stan [@stan_development_team_rstan:_2018] which performs sampling using Hamiltonian Monte Carlo.

[Note: the Berger et al. manuscript provides an approach for determining an 'overall objective prior' more systematically, but may be difficult to implement if interested in posterior parameters and functions of parameters (mean, CDF) simultaneously]

<!--
how concentrated the induced prior is around the MLEs for the cutpoints when $X=0$.-->

```{r probit-induced, fig.cap="Induced $\\boldsymbol{\\gamma}$ priors under $G^{-1}(\\cdot)=\\Phi^{-1}(\\cdot)$", fig.align='center', out.width='95%'}
knitr::include_graphics(file.path(wd,"fig","probit_induced.png"))
```


<!--
[possible choices Uniform 1, Jeffrey's 1/2, Objective?? $\alpha_j =\frac{1}{J}$ for all $j$, 1/(0.8 + 0.35 * max(n, 3))]. 

We explore several choices of prior with a goal of fin

(prior predictive distribution - first draw params from Dirichlet, then draw cdf vals see BDA3 p 7, plot ranks on x-axis and cdf val on y-axis)

Under conjugacy, the $\alpha_j$ can be interpreted as the number of pseudo-observations in each category contributed by the prior, so the choice of $\alpha_j =\frac{1}{J}$ implies a total prior contribution equivalent to 1 observation. <- what about Jacobian though


$\alpha=1$ equally favor all vectors $p_i$ such that $\sum p_i =1$
$\alpha < 1$ favors sparsity in most cells with high prob for small number of cells
$\alpha > 1$ favors equally distributed counts (If all concentration parameters are equal but greater than 1 then the prior mode is that the categories are equiprobable, and the larger the value of the identical concentration parameters, the more sharply peaked the distribution is at the mode. )

using a symmetric Dirichlet distribution so $\alpha_1=\alpha_2=\cdots=\alpha_n=1$ implies a posterior distribution of $p(\boldsymbol{p_i}|y) \propto Dirichlet(1+n_1,1+n_2,\ldots,1+n_J)$ where $n_j$ is the number of observations in each category.
-->




### Posterior Conditional Quantities

<!--
[description of how to use Bayesian CPM to get parameter ests. for shifts in distribution and other quantities of interest, e.g. conditional CDF Mean, Quantiles, using a single model]
-->

Using samples from the posterior distribution ($\tilde{\gamma}$, $\tilde{\beta}$) for the model defined in (\ref{eq:post}) it is straightforward to calculate the posterior conditional CDF, mean, quantiles or other functions of the parameters

- Posterior conditional CDF:  $\tilde{F}(y_i|\boldsymbol{x}_i)=G(\tilde{\gamma}_{r(y_i)}-\boldsymbol{x}_i^{T}\tilde{\boldsymbol{\beta}})$

- Posterior conditional mean:
$\tilde{E}[Y|\boldsymbol{x}_i]=\sum_{i=1}^{n}y_i\tilde{f}(y_i|\boldsymbol{x}_i)$ where $\tilde{f}(y_i|\boldsymbol{x}_i)=\tilde{F}(y_i|\boldsymbol{x}_i)-\tilde{F}(y_{i-1}|\boldsymbol{x}_i)$

- Posterior conditional quantile:
To estimate the $q^{th}$ quantile 
  - Find $y_i=\inf\{y:\tilde{F}(y|x)\ge q\}$ and the next smallest value $y_{i-1}$ 
  - Use linear interpolation to find quantile $y_q$ where $y_{i-1}<y_q<y_i$ 


<!---
### Nonparametric Model

empirical Bayesian histogram (e.g., Dirichlet process)

[comparison to other Bayesian nonparametric approach, e.g. Dirichlet process mixture or Polya trees]

The Dirichlet process prior is an infinite dimensional analogue to the Dirichlet prior (number of categories is as large as needed, although calculations are still finite dimensional)

[describe Dirichlet process prior, e.g. stick-breaking process and how it changes the model formulation]

 - what are advantages of CPM vs this model? - computation efficiency?? other? show connection to CPM (limiting case?)


Note: want to use BNP model that allows flexibility in the residual distribution for comparison
$y_i = f(x_i) + \epsilon_i$ with $\epsilon_i|G \stackrel{iid}\sim G$ with BNP for $G$ See Muller Chp 4, esp 4.2 and Chp 3. Try
$p(\boldsymbol{\pi_{\cdot}}) \sim DP(mG_0)$ with $G_0 \sim Unif(0,1)$ and $m$ specified to control concentration around $G_0$

For later models might want flexibility in $f(x_i)$ or completely nonparametric (i.e. density regression)
-->


<!--
### CPM for continuous outcomes

In the usual ordinal CPM described above, many of the $j=1,\ldots,J$ categories contain multiple observations, however for continuous data with no ties each category has only one observation. 

[Heuristic argument]
For continuous outcome $y$ with no ties, the dimension of the $\gamma$ parameter grows at the same rate as the sample size $n$, so an uniformative prior for $p(\boldsymbol{\gamma})$ will be highly influential on posterior inference.

- If use improper $\alpha=0$, revert to previous noninformative case and nonidentifiable unless additional constraint is added
- Using $\alpha=1$ which equally favors all vectors $p_i$ such that $\sum p_i=1$ adds as many observations from the prior as are observed (each cell has count 2, one from prior & one from observed data)

Rather than specifying concentration, estimate $\alpha$ distribution with $\alpha \sim Gamma(a, b)$.  !!!May need to use different distribution to make sure prior only adds a little info while still regularizing enough to estimate intercepts !!! (plot draws from this?) Still might have issues since implies all $\alpha_k$ are centered around same location, solve this problem by using dirichlet process mixture 

(<--need to bolster this argument!!, check lit, BDA3 etc.) (when all other params fixed, what does likelihood of say, $\gamma_2$ look like? show why uninformative prior won't work)

Hyp. For a low to moderate number of ordinal categories, both fixed conc and estimating conc perform similarly. [Add sentences/evidence to back this up. Maybe diagnostics/model fit (posterior predictive checks)]

Hyp. However, when the number of ordinal categories increases and the number of events per category is low, the fixed concentration parameter model (fails) to accurate recover the underlying [intercepts? distribution?].  

Hyp. there is some sort of shrinkage or regularization going on that is too strong with fixed parameter.

[Speculative statement: This implies that information supplied by the Dirichlet prior has higher weight since each cell is a combination of 'prior count' and a single observation - need to show this]. (Is Dir. with $\alpha=1$ the equivalent of noninformative prior for multinomial?? - see BDA3 chap on Bayesian histogram)

[General question, model is motivated by positing latent variables, e.g. conditional on covars; why not actually incorporate these latent variables using some sort of data augmentation or EM algorithm approach?]

change in likelihood for continuous?? 
\begin{gather*}
p(\boldsymbol{\gamma},\boldsymbol{\beta}|\boldsymbol{X},\boldsymbol{Y}) \propto p(\boldsymbol{\gamma},\boldsymbol{\beta})\prod_{i=1}^{n}[F(y_i|x_i)-F(y_i - |x_i)]
\end{gather*} 

where $F(y -|x)=\lim_{t \uparrow y}F(t|x)$

By default, $\alpha_i=1\, \forall i$ (i.e., prior count of 1 in each bin)
allow $\gamma \sim ??$ (ie hyperprior on concentration param)

add hyperprior so $\alpha \sim Gamma(a,b)$ (some sort of weakly informative, positive distribution such as centered around 1)

-->


## Simulations

To evaluate the long-run frequency properties of the Bayesian CPM we generate data from two simulation models. Both models begin with continuous data using:
$$Y=\exp(X_1\beta_1 + X_2 \beta_2 + \varepsilon) \quad \varepsilon \sim N(0,1)$$
where $\beta_1=1$, $\beta_2=-0.5$, $X_1 \sim Bernoulli(0.5)$ and $X_2 \sim N(0,1)$

The second model then sets values of $Y<0$ to 0 to produce a mixed discrete/continuous outcome simulating a lower limit of detection. $1000$ datasets were generated for sample sizes $n=25,50,100,200$ and $400$

A Bayesian CPM using three $\boldsymbol{\alpha}$ concentration hyperparameters for $p(\boldsymbol{\pi_{.}}|\boldsymbol{\alpha})$ with the properly specified probit link was fit to each simulated dataset across the two outcome models. <!-- For comparison, the NPMLE CPM model [and the BNP model for residuals] were also fit to each dataset. --> We examine the average percent bias of the posterior median parameters $\beta_1$ and $\beta_2$ and five posterior median $\gamma_j$ parameters corresponding to $y$ values $y_1=e^{-1},y_2=e^{-0.33},y_3=e^{0.5},y_4=e^{1.33},y_5=e^2$. We also calculate average percent bias for: the conditional CDF for the same $y$ values at $X_1=1$ and $X_2=1$ (Note: cdf at $X_1=1$ and $X_2=0$ not shown), the conditional median at $X_1=1$ and $X_2=1$, and for the uncensored outcome simulations, the conditional mean and 20$^{th}$ percentile. 

<!--
additional sims

case where likelihood not well approximated by quadratic function (ie NPMLE doesn't work well but Bayes CPM works)?

weird shapes (i.e. mixture distributions); mixture continuous/discrete (lower limit of detection) - see Yuqi's paper for ideas
-->

[Describe posterior predictive checks, Bayesian p-value or some additional Bayesian model checking]


[section describing how models were actually fit, i.e. used HMC implemented in Stan, JAGS, etc?]


## Case Study

<!--
Use HIV data from Dr. Koethe. Describe background, data and modeling approach
-->

The data for the case study were collected from 216 HIV-positive adults on antiretroviral therapy in two cohort studies. Further details on the study design and cohorts are provided in ???. Because people living with HIV have increased risk of diabetes and cardiovascular disease, the aim of the analysis is to estimate the association between body mass index (BMI) and several inflammation biomarkers in this population, adjusting for additional covariates (age, sex, race, smoking status, study location and CD4 cell count).

The biomarkers of interest are (IL-6) and IL-1-$\beta$. Both biomarkers are skewed and have values censored below a lower limit of detection. To account for skewness and censoring in the outcomes we fit a Bayesian CPM model using a probit link function to estimate the association between BMI and the conditional median, and 90th quantile of each biomarker.

# Results


## Simulations

Overall, the Bayesian CPM had relatively good performance under the simulation settings explored, especially for larger values of $n$. The three $\boldsymbol{\alpha}$ values produced similar results for most scenarios 


The average percent bias in the posterior median for $\beta_1$, $\beta_2$, and $\gamma_{y_k}$ are shown in figures \@ref(fig:simplt-pars1) and \@ref(fig:simplt-pars2) for the uncensored and censored outcome data, respectively. 

```{r simplt-pars1, fig.cap="Bias in parameters for uncensored simulations", fig.align='center', out.width='90%'}
knitr::include_graphics(file.path(wd,"fig","sim_param_full2.png"))
```

```{r simplt-pars2, fig.cap="Bias in parameters for censored simulations", fig.align='center', out.width='90%'}
knitr::include_graphics(file.path(wd,"fig","sim_param_cens2.png"))
```


Figures \@ref(fig:simplt-cdf1) and \@ref(fig:simplt-cdf2) show the bias in the posterior conditional CDF.

```{r simplt-cdf1, fig.cap="Bias in conditional CDF for uncensored simulations", fig.align='center', out.width='90%'}
knitr::include_graphics(file.path(wd,"fig","sim_cdf_full.png"))
```

```{r simplt-cdf2, fig.cap="Bias in conditional CDF for censored simulations", fig.align='center', out.width='90%'}
knitr::include_graphics(file.path(wd,"fig","sim_cdf_cens.png"))
```

The results for the conditional posterior median are shown in figures \@ref(fig:simplt-med1) and \@ref(fig:simplt-med2).

```{r simplt-med1, fig.cap="Bias in conditional median for uncensored simulations", fig.align='center', out.width='90%'}
knitr::include_graphics(file.path(wd,"fig","sim_med_full.png"))
```

```{r simplt-med2, fig.cap="Bias in conditional median for censored simulations", fig.align='center', out.width='90%'}
knitr::include_graphics(file.path(wd,"fig","sim_med_cens.png"))
```


Figure \@ref(fig:simplt-mn) presents the average bias in the posterior conditional mean for the uncensored simulation data. 

```{r simplt-mn, fig.cap="Bias in conditional mean for uncensored simulations", fig.align='center', out.width='90%'}
knitr::include_graphics(file.path(wd,"fig","sim_mn_full.png"))
```

Figure \@ref(fig:simplt-q20-1) presents the  results of the Bayesian CPM model for the uncensored simulation data 

```{r simplt-q20-1, fig.cap="Bias in conditional 20th percentile for uncensored simulations", fig.align='center', out.width='90%'}
knitr::include_graphics(file.path(wd,"fig","sim_q20_full.png"))
```


## Case Study 


Posterior parameter estimates for each of the biomarker outcomes in the HIV case study data are shown in \@ref(fig:il6-1) and \@ref(fig:il-1-beta-1). BMI is positively associated with increased IL-6

Add sensitivity analysis and other Bayesian modeling checks (convergence and model)

```{r il6-1, fig.cap='Posterior parameters for IL-6 model', fig.align='center', out.width='75%'}
knitr::include_graphics(file.path(wd,"fig","il_6_post.png"))
```

```{r il-1-beta-1, fig.cap='Posterior parameters for IL-1-$\\beta$ model', fig.align='center', out.width='75%'}
knitr::include_graphics(file.path(wd,"fig","il_1_beta_post.png"))
```


```{r il6-2, fig.align='center', out.width='75%'}
knitr::include_graphics(file.path(wd,"fig","il_6_bmi.png"))
```

```{r il-1-beta-2, fig.align='center', out.width='75%'}
knitr::include_graphics(file.path(wd,"fig","il_1_beta_bmi.png"))
```

# Discussion

Bayesian CPMs are a versatile modeling approach with many advantages:

- Avoid specification of outcome transformation 
- handle continuous and discrete ordered outcomes 
- estimate full conditional CDF, conditional mean and quantiles using a single model
- provide inference without asymptotic assumptions

## Limitations

- Bias for small $n$
- The number of distinct outcome values must be known *a priori*, that is we condition on $J$ categories. In practice, the number of distinct outcome values is unlikely to be known prior to data collection, so this formulation violates the principle that the prior should be specified without reference to the observed data.<!-- Some discussion of extensions using  e.g. Dirichlet process mixture or Polya trees (ref semiparameteric surv. analysis papers?) -->
- Computation time

## Extensions

- fully nonparametric priors, e.g. Dirichlet process mixture or Polya trees
- partial proportional odds 
- relaxation of link function specification

<!--

## Extending the CPM model

* Mixture link
-Not sure if mixture link is needed to fit model well. Evidence from previous papers and CPM poster show ok robustness, except for extreme link misspecification

One important component of the CPM is the specification of the link function $F(\cdot)$. Although the CPM model estimated using NPMLE is somewhat robust to link function misspecification it is of interest to determine the degree to which the data support one link function over another. In the Bayesian model, we can model a mixture of several link functions while incorporating the additional uncertainty inherent in allowing a combination of multiple links.

Consider both finite mixture $F_{mix}=w_1F_1 + w_2F_2 + (1-w_1-w_2)F_3$ and BNP prior over normal distributions to get nonparametric estimate of link 

[need to research more about mixture models]
[add Stan model implementing mixture link]
Let $F_1 \sim loglog$, $F_2 \sim logistic$, $F_3 \sim cloglog$ be three possible choices for link function. Then $F_{mix}=w_1F_1 + w_2F_2 + (1-w_1-w_2)F_3$ is a mixture. The priors for weights $w_1$, $w_2$, $w_3$ such that $\sum_i w_i = 1$

Incorporating this extension the full model becomes

$p(\boldsymbol{\gamma},\boldsymbol{\beta},\boldsymbol{w}|\boldsymbol{X},\boldsymbol{Y}) \propto p(\boldsymbol{\gamma},\boldsymbol{\beta}, \boldsymbol{w})\prod_{i=1}^{n}[F_{mix}(y_i|x_i)-F_{mix}(y_i - |x_i)]$

(do we need to get inverse?, i.e. $F_{mix}^{-1}(\cdot)$)

also consider using some sort of DP prior over normal distributions to get nonparametric estimate of link (lose interpretability and computational efficiency, but might have more robustness). One or the other might be better depending on the sample size, complexity of outcome, etc 

* PO, NPO, PPO extension
use selection for PO/NPO by variable -> see McKinley et al. [@mckinley_bayesian_2015]
or location-shift models -> see Tutz and Berger

* Nonlinear model extension - 
nonlinear in variables (splines etc) and nonlinear in parameters

* Longitudinal model extension - 
nonlinear mixed effects (will eventually be used for PK/PD modeling)

-->

<!--
### Paper ###

Bayes ordinal CPM for continuous outcome (i.e. extensions to prior spec to deal with large # of intercepts).

Use model with dirichlet and several concentration params for n = # distinct obs/categories
alpha=1/n
alpha=0? (NO! stan_polr and ord_mod1 don't work w/ improper alpha=0 conc. param)
alpha=1/2 (multinomial Jeffrey's prior)
alpha=1 ('uniform' prior)
alpha=1/(0.8 + 0.35 max(n, 3)) (conc param from Frank's sims)

compare Bayes CPM to NPMLE? - repro and compare orm() vignette, but not in paper

-> posterior predictive checks? Bayesian p-value or other Bayesian model checking

-> Case study with HIV data. Add model checking steps

TO DO
- add more semiparametric Bayesian model examples in intro

- justify/explain differences and contribution of Bayesian CPM vs. other semiparametric Bayesian models

- add more justification/recommendations for prior choice
-> explore overall objective prior approach of Berger to find/justify conc parameter


lower priority
-> figure out why hardcoded stan model doesn't match stan_polr when alpha=1, 0.5, 0.2, 2, 5, 1/ncat (other vals?),
-> change hardcoded stan model to est. cutpoints when all x are at their means (rather than all x=0) or mean-center all covars?
-> could also be related to parameterization of betas in stan_polr

### R package ###

TO DO

- add formula interface
- figure out Stan issue with non-symmetric link function
- add vignette using examples from orm() documentation
- note about centering predictors OR automatic centering of predictors (see brms and rstanarm which both do this)
- clean up & update CDF, Mean, Quantile, ExProb - to use one interface, maybe accept list like rms::contrast rather than newdata data.frame to facilitate comparisons?


lower priority
-> implement alternative CPM models
OpenBUGS
-- see http://www.omori.e.u-tokyo.ac.jp/WinBUGS/index.htm ex 14, 15, maybe 16
-- see https://gist.github.com/bartneck/038fae866e10641b8121679cd02db52e
-- see http://www.openbugs.net/Examples/Inhalers.html
-- Congdon book models

Other Stan models
e.g. Betancourt, Goodrich

MATLAB
mods from Johnson & Albert

PyMC3
https://discourse.pymc.io/t/ordered-probit-model-for-ordinal-data/1308


DONE
- verify priors are correct (Jacobian for induced dirichlet? See Betancourt ordinal regression blog post) -> solved, don't need Jacobian if using pw_log_lik function as in stan_polr, also see STAN User's guide 2.18.0 - 16.2 Reparameterizations and 16.3
--use roxygen function notation similar to 'Copula modeling with R'

-->


<!-- ## Background -->

<!-- mini lit review for ordinal models and summarizing results of previous continuous CPM papers -->

Cumulative models for ordinal outcomes -- traditionally denoted cumulative link models [@agresti_categorical_2002] -- have been discussed extensively in the literature using both classical (frequentist) and Bayesian implementations. Since these models are characterized by adding probabilities, not link functions, we prefer the nomenclature cumulative probability model (CPM). Under the frequentist paradigm, Walker and Duncan [@walker_estimation_1967] and McCullagh [@peter_mccullagh_regression_1980] described these models as an extension of dichotomous outcome regression models: logistic and probit regression are most well-known examples. A Bayesian CPM for ordinal regression was first detailed by Albert and Chib [@albert_bayesian_1993; @albert_bayesian_1997] and Johnson and Albert [@johnson_ordinal_1999]. <!--Peng and Hall [cite] describe a Bayesian approach to generalized ordinal regression models for rating data --> Additional Bayesian CPM extensions including partial proportional odds [@peterson_partial_1990], mixture link models [@lang_bayesian_1999], location-scale ordinal regression [citation] and multivariate ordinal outcomes are described by Congdon [@congdon_bayesian_2005]. In all these settings, the number of ordered outcome categories is implicitly assumed to be smaller -- often much smaller -- than the sample size. However, continuous data are also ordinal and can therefore be fit using CPMs.

In the continuous data setting, Liu et al. [@liu_modeling_2017] demonstrate the equivalence between CPMs and transformation models of the form:
\begin{equation}
Y=H(\beta^{T}X+\varepsilon) \quad \text{where} \quad \varepsilon \sim F_{\varepsilon}
\end{equation}
where $H(\cdot)$ is an increasing function, $\beta$ a vector of regression coefficients, $X$ a vector of covariates, and $\varepsilon$ are errors distributed according to $F_{\varepsilon}$. Following Zeng and Lin [], Harrell [@harrell_regression_2015] and Liu et al. [@liu_modeling_2017] describe approximate non-parametric maximum likelihood estimation (NPMLE)
for the unspecified transformation $H(\cdot)$ and $\beta$ parameters. The models have several favorable characteristics including invariance of regression coefficient estimates to monotonic outcome transformations and ability to handle mixed continuous and discrete outcomes such as those that arise from a lower or upper limit of detection. In addition CPMs directly model the full conditional cumulative distribution function (CDF); this allows estimates of conditional means, quantiles, and exceedance probabilities to be calculated from a single model fit. Further, because only the $H(\cdot)$ part of the model is nonparametric, CPMs are semiparametric regression models which balance the robustness of fully nonparametric models and the efficiency of fully parametric models.

There is an extensive literature on Bayesian semiparametric regression models. The aim -- stated by Gelfand [@gelfand_approaches_1999] in his discussion of general approaches for these models -- is, ``to enrich the class of standard parameteric hierarchical models by wandering nonparametrically near (in some sense) the standard class but retaining the linear structure.'' For example, Brunner [@brunner_bayesian_1995] describes Bayesian linear regression models with symmetric unimodal error densities and Kottas and Gelfand [@kottas_bayesian_2001] describe Bayesian semiparametric median regression [ADD DETAILS]. 
DeYoreo & Kottas [@deyoreo_bayesian_2020] explore Bayesian nonparametric density regression for ordinal responses by modeling the joint density between the outcome and covariates using latent continuous random variables.

Song & Lu [@song_semiparametric_2012] develop a semiparametric transformation nonlinear mixed model which estimates the transformation, $H(\cdot)$, and also incorporates possible nonlinear relationships between $X$ and $\beta$ as well as random effects using Bayesian P-splines. Tang et al. [@tang_semiparametric_2018] describe semiparametric Bayesian analysis for transformation linear mixed models using a similar Bayesian P-spline approach to estimate the transformation with a focus on nonparametric estimation of random effects. Additional details on general Bayesian nonparametric models can be found in the texts by Müller et al. [@muller_bayesian_2015] and Hjort et al. [@hjort_bayesian_2010]

In the context of survival data, Mallick & Walker [@mallick_bayesian_2003] describe a linear transformation model for mean survival time where the transformation, $H(\cdot)$ and the error distribution, $F_{\varepsilon}$, are estimated nonparametrically using mixtures of incomplete beta functions and a Pólya tree distribution, respectively. Lin et al. [@lin_semiparametric_2012] detail a semiparametric Bayesian transformation model for median survival. Hanson and colleagues [@damien_surviving_2013;@hanson_bayesian_2007] and the monograph by Ibrahim [@ibrahim_bayesian_2010] describe other Bayesian nonparametric survival models. 

<!-- DESCRIBE PROBLEMS/GAPS in literature using previous methods here and how we solve them
- contrasts and other functionals?
- incorporate prior info?
- similar strengths to Bayesian Survival - check there for addt'l benefits
-->

In this paper, we develop Bayesian CPMs for continuous and mixed outcomes. [They are distinguished from other Bayesian semiparametric approaches by their relationship to traditional ordinal regression models? something else?]. Bayesian CPMs inherit many of the properties of CPMs estimated using NPMLE and have additional benefits: interpretation using posterior probabilities, the ability to incorporate available prior information, and inference <!--when the log-likelihood is not well-approximated by a quadratic function [!! need to show example where this is the case in simulations]-->
for quantities of interest without using asymptotic approximations.
A primary challenge when implementing Bayesian CPMs for continuous outcomes is the specification of priors for the intercept parameters and we describe several proposed strategies. Through simulations, we explore characteristics of Bayesian CPMs using several model specifications and prior combinations<!--and make comparisons to CPM estimates using NPMLE and Bayesian nonparametric (BNP) regression, another popular approach for flexibly modeling outcomes-->. We present a case study involving HIV biomarker data for which the outcomes are skewed and censored at a lower limit of detection. We conclude with a discussion -- including advantages, current limitations, and extensions -- and recommendations for using Bayesian CPMs.

# Methods

## Cumulative Probability Model formulation
<!-- see Albert & Chib 1997 - sec 2, and Johnson & Albert Ordinal Data Modeling to clean up this section-->
Let $Y_i$ be the outcome for unit $i=1,\ldots,n$ with $p$ associated covariates $\boldsymbol{X_i}=(X_{i1},\ldots,X_{ip})$ such that each $Y_i$ falls into one of $j=1,\ldots, J$ ordered categories. The $Y_i$ can be modeled using a $Categorical(\boldsymbol{\pi_i})$ -- or $Multinomial(1,\boldsymbol{\pi_i})$ -- distribution where $\boldsymbol{\pi_i}=(\pi_{i1}, \ldots, \pi_{iJ})$ are the probabilities of unit $i$ being in category $j$ and $\sum_{j=1}^{J}\pi_{ij}=1$. The value of $\pi_{ij}$ is dependent on $x_i$, but we suppress the conditional notation except where it is needed for clarity. The cumulative probability of falling into category $j$ or a lower category is $Pr(Y_i \le j)=\eta_{ij}=\sum_{k=1}^{j}\pi_{ik}$. A CPM relates the cumulative probabilities to the observed covariates through a monotonically increasing link function $G^{-1}(\eta_{ij})=\gamma_{j}-\boldsymbol{x_i'\beta}$ or equivalently
\begin{gather}
Pr(Y \le j|X,\beta,\gamma)=\eta_{ij}=G(\gamma_{j}-\boldsymbol{x_i'\beta})
\end{gather}
Where the $\gamma_j$ are ordered continuous intercept parameters $-\infty \equiv \gamma_0 < \gamma_1 < \cdots < \gamma_{J-1} <\gamma_J \equiv \infty$ and $\boldsymbol{\beta}$ is a vector of $p$ coefficients.
For identifiability, the linear predictor $\boldsymbol{x_i'\beta}$ does not include an intercept, $\beta_0$. Common choices for $G^{-1}(\cdot)$ are logit, $G^{-1}(p)=\log\left(\frac{p}{1-p}\right)$; probit, $G^{-1}(p)=\Phi^{-1}(p)$; and loglog, $G^{-1}(p)=-\log(-\log(p))$. The conditional probabilities of category membership are
\begin{gather}
\label{eq:cellprobs}
\pi_{ij}=\eta_{i,j}-\eta_{i,j-1}=G(\gamma_j-\boldsymbol{x_i'\beta})-G(\gamma_{j-1}-\boldsymbol{x_i'\beta})
\end{gather}
Where the function $G(\cdot)$ is a CDF defined as the inverse of the link function: standard logistic, standard normal, and standard Gumbel for the logit, probit, and loglog links, respectively. The likelihood for an iid sample of observations $(y_1,\ldots,y_n)$ is
\begin{gather}
p(\boldsymbol{y}|\boldsymbol{x},\boldsymbol{\gamma},\boldsymbol{\beta})=
\prod_{j=1}^{J}\prod_{i:y_i=j}[G(\gamma_j-\boldsymbol{x_i'\beta})-G(\gamma_{j-1}-\boldsymbol{x_i'\beta})]
\end{gather}
For continuous data with no ties $J=n$. Letting $r(y_i)$ be the rank of $y_i$ the likelihood reduces to
\begin{gather}
p(\boldsymbol{y}|\boldsymbol{x},\boldsymbol{\gamma},\boldsymbol{\beta})=
\prod_{i=1}^{n}[G(\gamma_{r(y_i)}-\boldsymbol{x_i'\beta})-G(\gamma_{r(y_i)-1}-\boldsymbol{x_i'\beta})]
\end{gather}

To complete the model specification we define priors for the parameters $p(\boldsymbol{\beta},\boldsymbol{\gamma})$. We assume *a priori* independence between $\boldsymbol{\beta}$ and $\boldsymbol{\gamma}$ so $p(\boldsymbol{\beta},\boldsymbol{\gamma})=p(\boldsymbol{\beta})p(\boldsymbol{\gamma})$. To simplify the model formulation we also assume noninformative priors for the regression coefficients, $p(\boldsymbol{\beta}) \propto \boldsymbol{1}$ however weakly informative or informative priors can also be used. <!--conditional means approach for defining joint prior fro Johnson & ALbert sec 4.2.2 pp 132 -->

Specifying priors for $\boldsymbol{\gamma}$ is more challenging because of the ordering restriction. Several approaches have been suggested in the traditional CPM setting where $J \ll n$. McKinley et al. [@mckinley_bayesian_2015] and Congdon [@congdon_bayesian_2005] describe a sequentially truncated prior distribution: $p(\boldsymbol{\gamma})=p(\gamma_1)\prod_{j=2}^{J-1}p(\gamma_i|\gamma_{j-1})$ where the support of $p(\gamma_1)$ is $\mathbb{R}$ and the support of $p(\gamma_j)$ for $j=2,\ldots, J-1$ is $(\gamma_{j-1},\infty)$. For example using normal and truncated normal priors, $p(\gamma_1)\sim N(0, \sigma_\gamma^2)$ and $p(\gamma_j|\gamma_{j-1}) \sim N(0, \sigma_\gamma^2)I(\gamma_{j-1},\infty)$. A second approach described by Albert and Chib [@albert_bayesian_1997] defines the prior on a transformation of the intercepts to an unconstrained space; normalizing $\gamma_0$ to 0 so $0 \equiv \gamma_0 < \gamma_1 < \cdots < \gamma_{J-1} <\gamma_J \equiv \infty$ and letting $\delta_1=\log(\gamma_1)$ and $\delta_j=\log(\gamma_j - \gamma_{j-1}),\, 2 \le j \le J-1$ a multivariate prior can be assigned, e.g. $\boldsymbol{\delta} \sim N_{J-1}(\boldsymbol{\mu_0},\boldsymbol{\Sigma_0})$.
Both approaches provide priors that satisfy the ordering restriction, but may be cumbersome when the number of distinct continuous data values is high. The first requires specification of the distribution and its hyperparameters, then sampling from the sequential series of $J-2$ truncated distributions; the second requires specification of the $J-1$ dimensional $\boldsymbol{\mu_0}$ vector and the $J-1 \times J-1$ covariance matrix $\boldsymbol{\Sigma_0}$.


We instead adopt a third approach which defines a prior on $\boldsymbol{\pi_i}$ for a prespecified covariate vector and utilizes the transformation defined by $G(\cdot)$ to induce a prior on the $\boldsymbol{\gamma}$ [@betancourt_ordinal_2019].
Let $\pi_{.j} \equiv Pr(r(y)=j|x=0)$ be the probability of being in category $j$ when all covariates are 0 (or at their mean values if covariates have been centered) and $\boldsymbol{\pi_{.}}=(\pi_{.1},\ldots,\pi_{.J})$. From equation (\ref{eq:cellprobs}) it follows that
\begin{gather}
\pi_{.j}=G(\gamma_j-0)-G(\gamma_{j-1}-0)=G(\gamma_j)-G(\gamma_{j-1})
\end{gather}
These equations define a transformation $h(\boldsymbol{\gamma})=\boldsymbol{\pi_{.}}$ between the intercept parameters and probabilities of category membership when $\boldsymbol{X}=\boldsymbol{0}$. Conversely,
\begin{gather}
\sum_{k=1}^{j}\pi_{.k}=\sum_{k=1}^{j}\left[G(\gamma_k)-G(\gamma_{k-1})\right]=G(\gamma_j)
\end{gather}
so $G^{-1}\left(\sum_{k=1}^{j}\pi_{.k}\right)=\gamma_j$ defines the inverse transformation.

Because $\boldsymbol{y}$ has a multinomial distribution a conjugate Dirichlet distribution with hyperparameters $\boldsymbol{\alpha}$ is a natural choice of prior for $\boldsymbol{\pi_{.}}$. Setting $p(\boldsymbol{\pi_{.}}|\boldsymbol{\alpha}) \propto \prod_{j=1}^{J}\pi_{.j}^{\alpha_j-1}$ the posterior distribution is
\begin{align}
p(\boldsymbol{\gamma},\boldsymbol{\beta}|\boldsymbol{x},\boldsymbol{y}) & \propto p(\boldsymbol{\gamma})p(\boldsymbol{\beta}) p(\boldsymbol{y}|\boldsymbol{x},\boldsymbol{\gamma},\boldsymbol{\beta})\\
&\propto p(h(\boldsymbol{\gamma}))|\mathcal{J}|p(\boldsymbol{\beta}) p(\boldsymbol{y}|\boldsymbol{x},\boldsymbol{\gamma},\boldsymbol{\beta})\\
\label{eq:post}
&\propto p(\boldsymbol{\pi_{\cdot}}|\boldsymbol{\alpha})|\mathcal{J}|p(\boldsymbol{\beta}) p(\boldsymbol{y}|\boldsymbol{x},\boldsymbol{\gamma},\boldsymbol{\beta})
\end{align}
where $\mathcal{J}$ is the Jacobian of the transformation $h(\boldsymbol{\gamma})=\boldsymbol{\pi_{.}}$. Letting $\Omega=\sum_{j=1}^J\pi_{.j}=1$ be the constraint that all category probabilites sum to 1, the entries in $\mathcal{J}$ where $\mathcal{J}_{r,c}$ is the term in row $r$ and column $c$ are
\begin{equation*}
\mathcal{J}_{j,1}=\frac{\partial \pi_{.j}}{\partial \Omega}=1 \quad
\mathcal{J}_{j+1,j+1}=\frac{\partial \pi_{.j+1}}{\partial \gamma_j}=\frac{\partial}{\partial \gamma_j}\left[G(\gamma_{j+1})-G(\gamma_{j})\right]=-g(\gamma_j) \quad
\mathcal{J}_{j,j+1}=\frac{\partial \pi_{.j}}{\partial \gamma_j}=\frac{\partial}{\partial \gamma_j}\left[G(\gamma_{j})-G(\gamma_{j-1})\right]=g(\gamma_j)
\end{equation*}
Where $j=1,\ldots,J-1$, $g(\cdot)$ is the pdf of the distribution $G(\cdot)$, and $\mathcal{J}_{r,c}=0$ for all other entries; the form of the Jacobian is

<!--
 \begin{vmatrix}
 \mathcal{J}_{11} & \mathcal{J}_{12} & \mathcal{J}_{13} & \mathcal{J}_{14}\\
 \mathcal{J}_{21} & \mathcal{J}_{22} & \mathcal{J}_{23} & \mathcal{J}_{24}\\
 \mathcal{J}_{31} & \mathcal{J}_{32} & \mathcal{J}_{33} & \mathcal{J}_{34}\\
 \mathcal{J}_{41} & \mathcal{J}_{42} & \mathcal{J}_{43} & \mathcal{J}_{44}\\
 \end{vmatrix}\\
 =
-->
\begin{equation}
\begin{vmatrix}
1      &  g(\gamma_1)  &  0            & 0           & \cdots           & 0 \\
1      & -g(\gamma_1)  &  g(\gamma_2)  & 0           & \cdots           & 0 \\
1      & 0             & -g(\gamma_2)  & g(\gamma_3) & \cdots           & 0 \\
\vdots & \vdots        &  \vdots       & \vdots      & \ddots           & \vdots \\
1      & 0             &  0            & 0           & -g(\gamma_{j-1}) & g(\gamma_j)\\
1      & 0             &  0            & 0           & 0                & -g(\gamma_j)\\
\end{vmatrix}
\end{equation}

While it is possible to define separate $\alpha_j$ parameters for each category, we restrict our attention to symmetric Dirichlet distributions which use a single $\alpha$ value for all categories (i.e., $\alpha_1 = \alpha_2 = \cdots = \alpha_J$). The symmetric Dirichlet prior on $\boldsymbol{\pi_{.}}$ along with the transformation $h(\cdot)$ induces a prior for the $\boldsymbol{\gamma}$ with $\boldsymbol{\alpha}$ controlling the concentration of the induced prior. Figure \@ref(fig:probit-induced) shows induced $\boldsymbol{\gamma}$ priors assuming a probit link for several combinations of concentration parameter and number of categories. The priors are approximately distributed around the intercepts that result under an assumption of equal probability for all categories when $\boldsymbol{\beta}=\boldsymbol{0}$; that is, for $J=n$ the values $G^{-1}(\sum_{k=1}^j 1/n)=G^{-1}(j/n)=\hat{\gamma}_{j|X=0}$. <!--multinomial-Dirichlet in sec 3.4 of BDA3 -->The choices correspond to several options for a multinomial-Dirichlet model [@gelman_bayesian_2014]: a uniform Dirichlet ($\boldsymbol{\alpha}=1$), the multivariate Jeffrey's prior ($\boldsymbol{\alpha}=1/2$), an overall objective prior recommended by Berger et al. [@berger_overall_2015] ($\boldsymbol{\alpha}=1/J$), and two additional reciprocal formulas ($\boldsymbol{\alpha}=1/(2+(J/3))$ and $\boldsymbol{\alpha}=1/(0.8+0.35J)$). As the number of categories increases the fixed $\boldsymbol{\alpha}$ priors more strongly favor intercepts assuming equal probability for all categories; the reciprocal priors are adjusted to maintain the same degree of concentration relative to the equal probability intercepts.

In multiparameter models, the choice of reference prior depends on the parameter or statistic of interest (e.g. $\beta$, conditional CDF, conditional mean) [@berger_overall_2015].  Without prior information, we seek a value of $\boldsymbol{\alpha}$ with minimal impact on inference for a variety of settings and quantities of interest while still producing posterior estimates that can be well sampled by the MCMC algorithm.

We evaluate convergence of each model using Rhat (name? Gelman-Rubin??) [cite] and traceplots. Model comparison is performed using leave-one-out (loo) information criteria. Model fit is assessed using graphical checks the of posterior predictive and posterior predictive p-values [cite Gelman? Stern & Sinharay].

<!--
Model checking
- evaluate model fit with posterior predictive checks, prob-scale residuals, latent var residuals, other??
- comparison of posterior stats, e.g. DIC, loo-IC etc
- plot of proportional odds/parallelism assumption?? could use something like smoother described in Congdon to do this
- is there a test statistic and corresponding Bayesian p-value that makes sense in this setting? check Bayes notes and BDA3 
-->

<!-- section describing how models were actually fit, i.e. used HMC implemented in Stan -->
The model in (\ref{eq:post}) is implemented using the `R` interface to Stan [@stan_development_team_rstan:_2018] which performs MCMC sampling using Hamiltonian Monte Carlo. [Add a few sentences about HMC - cite Turkman Computational Bayesian Statistics?] number iterations, number of chains, burn-in, adapt delta & other settings

<!-- describe how to implement model with a more traditional Metropolis-Hastings algorithm?? -->

[Note: the Berger et al. manuscript provides an approach for determining an 'overall objective prior' more systematically, but may be difficult to implement if interested in posterior parameters and functions of parameters (mean, CDF, quantiles) simultaneously]

marginal reference prior for $\gamma_1=G^{-1}(\pi_{.1}) \Rightarrow G(\gamma_1)=\pi_{.1}$. want $\gamma_1 \sim U(-\infty, \infty)$. marginal dist of $\pi_{.1} \sim Beta(a,b)$

```{r, eval=F}
hist(qlogis(rbeta(500,1,1)))
hist(qlogis(rbeta(500,1/2,1/2)))
hist(qlogis(rbeta(500,0.01,0.01)))
# limit as Beta params a->0 and b -> 0 gives noninformative prior for gamma
```

reference prior for $\gamma_2=G^{-1}(\pi_{.1}+\pi_{.2})$. need constraint that $\pi_{.1}+\pi_{.2}\le 1$
```{r, eval=F}
hist(runif(500)+runif(500)) # triangular dist.
hist(rbeta(500,1/2,1/2)+rbeta(500,1/2,1/2))
hist(rbeta(5000,1/4,1/4)+rbeta(5000,1/4,1/4))
hist(rbeta(5000,1/6,1/6)+rbeta(5000,1/6,1/6))

hist(qlogis(rbeta(5000,1/6,1/6)+rbeta(5000,1/6,1/6)))
```

reference prior for $\gamma_2=G^{-1}(\pi_{.1}+\pi_{.2})$

```{r probit-induced, fig.cap="Induced $\\boldsymbol{\\gamma}$ priors under $G^{-1}(\\cdot)=\\Phi^{-1}(\\cdot)$", fig.align='center', out.width='95%'}
knitr::include_graphics(file.path(wd,"fig","probit_induced_mle.png"))
```

### Posterior Conditional Quantities

<!--
[description of how to use Bayesian CPM to get parameter ests. for shifts in distribution and other quantities of interest, e.g. conditional CDF Mean, Quantiles, using a single model]
-->

Using MCMC samples from the posterior distribution ($\tilde{\gamma}$, $\tilde{\beta}$) it is straightforward to calculate the posterior conditional CDF, mean, quantiles or other functions of the parameters

- Posterior conditional CDF:  $\tilde{F}(y_i|\boldsymbol{x}_i)=G(\tilde{\gamma}_{r(y_i)}-\boldsymbol{x}_i^{T}\tilde{\boldsymbol{\beta}})$

- Posterior conditional mean:
$\tilde{E}[Y|\boldsymbol{x}_i]=\sum_{i=1}^{n}y_i\tilde{f}(y_i|\boldsymbol{x}_i)$ where $\tilde{f}(y_i|\boldsymbol{x}_i)=\tilde{F}(y_i|\boldsymbol{x}_i)-\tilde{F}(y_{i-1}|\boldsymbol{x}_i)$

mean only makes sense with continuous data, data below or above a detection limit do not have a known value of $y_i$ that can be used in this calculation without making additional assumptions about the distribution

- Posterior conditional quantile:
To estimate the $q^{th}$ quantile
  - Find $y_i=\inf\{y:\tilde{F}(y|x)\ge q\}$ and the next smallest value $y_{i-1}$
  - Use linear interpolation to find quantile $y_q$ where $y_{i-1}<y_q<y_i$

## Simulations

To evaluate the <!--long-run frequency--> properties of the Bayesian CPM we generate data from several simulation models:
\begin{align*}
Y&=\exp(X_1\beta_1 + X_2 \beta_2 + \varepsilon) \quad \varepsilon \sim N(0,1)\\
Y&=\exp(X_1\beta_1 + X_2 \beta_2 + \varepsilon) \quad \varepsilon \sim Logistic(0,1/3)\\
Y&=X_1\beta_1 + X_2 \beta_2 + \varepsilon \quad \varepsilon \sim Gumbel(0,1)
\end{align*}
where $\beta_1=1$, $\beta_2=-0.5$, $X_1 \sim Bernoulli(0.5)$ and $X_2 \sim N(0,1)$

For each setting a second set of simulations is used to evaluate a mixed discrete/continuous outcome such as a lower limit of detection. For the first two scenarios values of $Y<1$ to were set to 1 and for the third scenario values of $Y<0$ were set to 0. Under each of the six scenarios, $1,000$ datasets were generated for sample sizes $n=25,50,100,200$ and $400$.

!! update this paragraph !!
A Bayesian CPM using three $\boldsymbol{\alpha}$ concentration hyperparameters for $p(\boldsymbol{\pi_{.}}|\boldsymbol{\alpha})$ with the properly specified probit link was fit to each simulated dataset across the two outcome models. We examine the average percent bias of the posterior median for parameters $\beta_1$ and $\beta_2$ and five posterior median $\gamma_j$ parameters corresponding to $y$ values $\boldsymbol{y}=\{y_1=e^{-1},y_2=e^{-0.33},y_3=e^{0.5},y_4=e^{1.33},y_5=e^2\}$. We also calculate average percent bias of the conditional CDF for $\boldsymbol{y}$ when $X_1=1$ and $X_2=1$, the conditional median at $X_1=1$ and $X_2=1$, and the conditional mean and 20$^{th}$ percentile for the uncensored outcome simulations.

<!-- compare using Bias, RMSE, coverage?, other statistics? -->

<!--
Additional sims

case where likelihood not well approximated by quadratic function (ie NPMLE doesn't work well but Bayes CPM works)?

weird shapes (i.e. mixture distributions); mixture continuous/discrete (lower limit of detection) - see Yuqi's paper and Song paper for ideas

try more complex mix of continuous and categorical (e.g. categorical below threshold and above threshold? may not work without modified multiple detection limit likelihood)
-->



## Case Study

<!-- Use HIV data from Dr. Koethe. Describe background, data and modeling approach -->

The data for the case study were collected from 216 HIV-positive adults on antiretroviral therapy in two cohort studies. Further details on the study design and cohorts are provided in Koethe et al. [@koethe_serum_2012; @koethe_metabolic_2015]. Because people living with HIV have increased risk of diabetes and cardiovascular disease, the aim of the analysis is to estimate the association between body mass index (BMI) and several inflammation biomarkers in this population, adjusting for additional covariates: age, sex, race, smoking status, study location and CD4 cell count.

The biomarkers of interest are Interleukin 6 (IL-6) and Interleukin 1 beta (IL-1$\beta$). Both biomarkers are skewed with values censored below a lower limit of detection set to 0. To account for skewness and censoring we fit Bayesian CPM models using logit, probit, and loglog link functions, noninformative $\beta$ priors and a concentration parameter of $\alpha=1/n$ for the Dirichlet distribution to estimate the association between BMI and the conditional mean, median, and 90th quantile of each biomarker.

# Results


## Simulations

In general, the Bayesian CPM had reasonable performance under the simulation settings explored; this is especially true for larger sample sizes. However, performance was poor for some quantities. The three $\boldsymbol{\alpha}$ values produced similar results for most scenarios

The average percent bias in the posterior median for $\beta_1$, $\beta_2$, and $\gamma_{y_k}$ are shown in figures \@ref(fig:simplt-pars1) and \@ref(fig:simplt-pars2) for the uncensored and censored outcome data, respectively.

```{r simplt-pars1, fig.cap="Bias in parameters for uncensored simulations using probit link", fig.align='center', out.width='90%'}
knitr::include_graphics(file.path(wd,"fig","sim_a_pars_full.png"))
```

```{r simplt-pars2, fig.cap="Bias in parameters for censored simulations using probit link", fig.align='center', out.width='90%'}
knitr::include_graphics(file.path(wd,"fig","sim_a_pars_cens.png"))
```

Figures \@ref(fig:simplt-cdf1) and \@ref(fig:simplt-cdf2) show the bias in the posterior conditional CDF.

```{r simplt-cdf1, fig.cap="Bias in conditional CDF for uncensored simulations using probit link", fig.align='center', out.width='90%'}
knitr::include_graphics(file.path(wd,"fig","sim_b_cdf_full.png"))
```

```{r simplt-cdf2, fig.cap="Bias in conditional CDF for censored simulations using probit link", fig.align='center', out.width='90%'}
knitr::include_graphics(file.path(wd,"fig","sim_b_cdf_cens.png"))
```

The results for the conditional posterior median are shown in figures \@ref(fig:simplt-med1) and \@ref(fig:simplt-med2).

```{r simplt-med1, fig.cap="Bias in conditional median for uncensored simulations using probit link", fig.align='center', out.width='90%'}
knitr::include_graphics(file.path(wd,"fig","sim_b_med_full.png"))
```

```{r simplt-med2, fig.cap="Bias in conditional median for censored simulations using probit link", fig.align='center', out.width='90%'}
knitr::include_graphics(file.path(wd,"fig","sim_b_med_cens.png"))
```

Figure \@ref(fig:simplt-mn1) presents the average bias in the posterior conditional mean for the uncensored simulation data and \@ref(fig:simplt-mn2) shows censored data.

```{r simplt-mn1, fig.cap="Bias in conditional mean for uncensored simulations using probit link", fig.align='center', out.width='90%'}
knitr::include_graphics(file.path(wd,"fig","sim_b_mn_full.png"))
```

```{r simplt-mn2, fig.cap="Bias in conditional mean for censored simulations using probit link", fig.align='center', out.width='90%'}
knitr::include_graphics(file.path(wd,"fig","sim_b_mn_cens.png"))
```

Figure \@ref(fig:simplt-q20-1) presents the results for conditional 20th percentile in the uncensored simulation data.

```{r simplt-q20-1, fig.cap="Bias in conditional 20th percentile for uncensored simulations using probit link", fig.align='center', out.width='90%'}
knitr::include_graphics(file.path(wd,"fig","sim_b_q20_full.png"))
```

```{r simplt-q20-2, fig.cap="Bias in conditional 20th percentile for censored simulations using probit link", fig.align='center', out.width='90%'}
knitr::include_graphics(file.path(wd,"fig","sim_b_q20_cens.png"))
```

### Computation time

Make script to extract time from sim *.out files

```{r simplt-samptime1, fig.cap="Per chain MCMC sampling time for scenario 1", fig.align='center', out.width='90%'}
knitr::include_graphics(file.path(wd,"fig","sim_a_MCMC_sample_times.png"))
```

```{r simplt-samptime2, fig.cap="Per chain MCMC sampling time for scenario 1 - 2nd run", fig.align='center', out.width='90%'}
knitr::include_graphics(file.path(wd,"fig","sim_b_MCMC_sample_times.png"))
```

```{r simplt-samptime3, fig.cap="Per chain MCMC sampling time for scenario 2", fig.align='center', out.width='90%'}
knitr::include_graphics(file.path(wd,"fig","sim_c_MCMC_sample_times.png"))
```

```{r simplt-samptime4, fig.cap="Per chain MCMC sampling time for scenario 3", fig.align='center', out.width='90%'}
knitr::include_graphics(file.path(wd,"fig","sim_d_MCMC_sample_times.png"))
```

Sims were run with R version 3.6.0 (2019-04-26) and rstan (Version 2.19.2, GitRev: 2e1f913d3ca3) on a high-performance computing cluster running under CentOS Linux 7 (Core)
with Intel(R) Xeon(R) CPU E5-2630 v3 @ 2.40GHz (might be diff for some nodes, extract info & summarize), amount RAM?

## Case Study

For the IL-6 outcome ...,

for IL-1-$\beta$ 

Posterior covariate parameter estimates for each of the biomarker outcomes in the HIV case study data are shown in \@ref(fig:il6-par) and \@ref(fig:il-1-beta-par). BMI is positively associated with increased IL-6, but not associated with increased IL-1-$\beta$.

### IL-6 biomarker

MCMC checks - traceplots and Rhat (supp) 

model comparison using loo (supp)


```{r il6-postpred, fig.cap='Observed and posterior predictive distribution for IL-6 model', fig.align='center', out.width='75%'}
knitr::include_graphics(file.path(wd,"fig","il_6_postpred.png"))
```

```{r il6-par, fig.cap='Posterior $\\beta$ estimates for IL-6 model', fig.align='center', out.width='75%'}
knitr::include_graphics(file.path(wd,"fig","il_6_post.png"))
```

```{r il6-par-ct, fig.cap='Posterior $\\gamma$ estimates for IL-6 model', fig.align='center', out.width='75%'}
knitr::include_graphics(file.path(wd,"fig","il_6_post_ct.png"))
```

```{r il-6-trans-1, fig.cap='Estimated transformation for IL-6 model using centered covariates', fig.align='center', out.width='75%'}
knitr::include_graphics(file.path(wd,"fig","il_6_trans.png"))
```

```{r il-6-trans-2, fig.cap='Estimated transformation for IL-6 model using uncentered covariates', fig.align='center', out.width='75%'}
knitr::include_graphics(file.path(wd,"fig","il_6_trans_nocnt.png"))
```

```{r il6-bmi, fig.align='center', out.width='75%'}
knitr::include_graphics(file.path(wd,"fig","il_6_bmi.png"))
```

### IL-1-beta biomarker

MCMC checks - traceplots and Rhat (supp) 

model comparison using loo (supp)

```{r il-1-beta-postpred, fig.cap='Observed and posterior predictive distribution for IL-1-$\\beta$ model', fig.align='center', out.width='75%'}
knitr::include_graphics(file.path(wd,"fig","il_1_beta_postpred.png"))
```


```{r il-1-beta-par, fig.cap='Posterior $\\beta$ estimates for IL-1-$\\beta$ model', fig.align='center', out.width='75%'}
knitr::include_graphics(file.path(wd,"fig","il_1_beta_post.png"))
```

```{r il-1-beta-par-ct, fig.cap='Posterior $\\gamma$ estimates for IL-1-$\\beta$ model', fig.align='center', out.width='75%'}
knitr::include_graphics(file.path(wd,"fig","il_1_beta_post_ct.png"))
```


```{r il-1-beta-trans, fig.cap='Estimated transformation for IL-1-$\\beta$ model', fig.align='center', out.width='75%'}
knitr::include_graphics(file.path(wd,"fig","il_1_beta_trans.png"))
```

```{r il-1-beta-bmi, fig.align='center', out.width='75%'}
knitr::include_graphics(file.path(wd,"fig","il_1_beta_bmi.png"))
```

<!--
Add sensitivity analysis and other Bayesian modeling checks (convergence and model)
-->

# Discussion

Bayesian CPMs are a versatile modeling approach with many advantages:

- Avoid specification of outcome transformation
- handle continuous and discrete ordered outcomes
- estimate full conditional CDF, quantiles and mean using a single model fit
- provide inference without asymptotic assumptions

## Limitations


- assumes parallelism (constant difference between transformed conditional CDFs, i.e. proportional odds assumption if using logit link or proportional hazards assumption for cloglog), however less
- The number of distinct outcome values must be known *a priori*, that is we condition on $J$ categories. In practice, the number of distinct continuous outcome values is unlikely to be known before data collection, so this formulation violates the principle that the prior should be specified without reference to the observed data. For this same reason, the model cannot accomodate new observations so is not useful for Bayesian updating or adaptive designs. In other words, once the initial prior is set the posterior from an initial model cannot serve as the prior for a model with additional data points because there is no way to add categories. <!-- Some discussion of extensions using  e.g. Dirichlet process mixture or Polya trees (ref semiparameteric surv. analysis papers?) -->
- The choice of link function, and the implied error distribution on the scale of the untransformed data is also assumed to be known. As was seen in the simulations, misspecification of the link function can lead to large bias.

- For both the Bias for small $n$
- Computation time

## Extensions

- nonparametric priors, e.g. Dirichlet process mixture or Polya trees
- partial proportional odds
- relaxation of link function specification

<!--

## Extending the CPM model

* Mixture link
-Not sure if mixture link is needed to fit model well. Evidence from previous papers and CPM poster show ok robustness, except for extreme link misspecification

One important component of the CPM is the specification of the link function $F(\cdot)$. Although the CPM model estimated using NPMLE is somewhat robust to link function misspecification it is of interest to determine the degree to which the data support one link function over another. In the Bayesian model, we can model a mixture of several link functions while incorporating the additional uncertainty inherent in allowing a combination of multiple links.

Consider both finite mixture $F_{mix}=w_1F_1 + w_2F_2 + (1-w_1-w_2)F_3$ and BNP prior over normal distributions to get nonparametric estimate of link

[need to research more about mixture models]
[add Stan model implementing mixture link]
Let $F_1 \sim loglog$, $F_2 \sim logistic$, $F_3 \sim cloglog$ be three possible choices for link function. Then $F_{mix}=w_1F_1 + w_2F_2 + (1-w_1-w_2)F_3$ is a mixture. The priors for weights $w_1$, $w_2$, $w_3$ such that $\sum_i w_i = 1$

Incorporating this extension the full model becomes

$p(\boldsymbol{\gamma},\boldsymbol{\beta},\boldsymbol{w}|\boldsymbol{X},\boldsymbol{Y}) \propto p(\boldsymbol{\gamma},\boldsymbol{\beta}, \boldsymbol{w})\prod_{i=1}^{n}[F_{mix}(y_i|x_i)-F_{mix}(y_i - |x_i)]$

(do we need to get inverse?, i.e. $F_{mix}^{-1}(\cdot)$)

also consider using some sort of DP prior over normal distributions to get nonparametric estimate of link (lose interpretability and computational efficiency, but might have more robustness). One or the other might be better depending on the sample size, complexity of outcome, etc

* PO, NPO, PPO extension
use selection for PO/NPO by variable -> see McKinley et al. [@mckinley_bayesian_2015]
or location-shift models -> see Tutz and Berger

* Nonlinear model extension -
nonlinear in variables (splines etc) and nonlinear in parameters

* Longitudinal model extension -
nonlinear mixed effects (will eventually be used for PK/PD modeling)

-->

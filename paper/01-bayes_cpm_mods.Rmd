
<!--
Paper

1a) start by writing up Bayes ordinal CPM for continuous outcome (i.e. extensions to prior spec to deal with large # of intercepts). 

(I think the main modification needed for this extension will be a hierarchical structure (hyperparameter) on the Dirichlet prior concentration parameters. NO, scratch this for now)

-> Use model with dirichlet and concentration alpha=1/n  (should n be # obs or # unique obs)
also tried:
alpha=0 (NO! stan_polr and ord_mod1 don't work w/ improper alpha=0 conc. param)
alpha=1/2 (I think this is multinomial Jeffrey's prior)
alpha=1

lower priority -> parameterization on differences in intercepts or log difference in intercepts

lower priority -> 1b) figure out why hardcoded stan model doesn't match stan_polr when alpha=1, 0.5, 0.2, 2, 5, 1/ncat (other vals?), 
-> maybe change hardcoded stan model to est. cutpoints when all x are at their means (rather than all x=0 which I think is in implementation as of 10/10/19)
-> could also be related to parameterization of betas in stan_polr

1c) simulations comparing Bayes CPM to NPMLE (maybe not in paper, but need to verify models are similar)

1d) simulations comparing Bayes CPM to Bayesian nonparametric (BNP) model

1e) Case study with HIV data

For next paper - extensions

2a) Non-proportional odds. extend to weighted PPO vs. NPO type model (McKinley), except using continuous data and mixture links. [from Frank] consider adding a summarization of evidence for the candidate link functions, e.g., posterior probability that the model has a logit link.  And develop ways to summarize exposure/treatment effects in these mixtures of models, e.g. covariate-specific estimates of both odds ratios and risk differences, plus perhaps hazard ratio.

2b) mixture link model (maybe, depends on whether it's truly needed or improves model)
--finite mixture and BNP mixture over, e.g. normals
-- see brms mixture() and BDA3

2c) Extend to nonlinear model - try something simple like Emax
-- see https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html
-- see http://mc-stan.org/rstanarm/articles/glmer.html
-- see https://mc-stan.org/rstanarm/reference/stan_nlmer.html

2d) Extend to mixed effects CPMs - nonlinear mixed effects model for Bayesian PK/PD

TO DO
- Clean up section describing basic Bayesian ordinal model

- Write section on extension to continuous, nail down 'proof' related to priors for intercepts/heuristic argument?
-- verify that NPMLE likelihood and my likelihood are the same

-->

<!--
R package
- add formula interface
- automatic centering of predictors (see brms and rstanrm which both do this)
- update CDF, Mean, Quantile to accept list like rms::contrast rather than newdata data.frame to facilitate comparisons
- figure out Stan issue with non-symmetric link function 
- clean up functions to get CDF, Mean, Quantile and Exceedance from Stan output 
- add vignette using all examples from orm() documentation
- verify priors are correct (Jacobian for induced dirichlet? See Betancourt ordinal regression blog)
--use roxygen function notation similar to 'Copula modeling with R' - DONE

- code for basic simulations
-- extend to more iterations or other scenarios (see Yuqi and Qi papers?) 
-- make sure model is working well in basic scenario; compare to NPMLE
using Bias, coverage, other statistics?

- implement BNP model for comparison (JAGS? OpenBUGS? dirichletprocess package?, other?)
-- see https://cran.r-project.org/web/packages/dirichletprocess/index.html
https://cran.r-project.org/web/packages/BNSP/index.html
https://www.stats.ox.ac.uk/~teh/research/npbayes/Teh2010a.pdf

lower priority -> implement CPM models in OpenBUGS??
-- see http://www.omori.e.u-tokyo.ac.jp/WinBUGS/index.htm ex 14, 15, maybe 16
-- see https://gist.github.com/bartneck/038fae866e10641b8121679cd02db52e
-- see http://www.openbugs.net/Examples/Inhalers.html

lower priority -> write code for mixed effects model
-- see Sorensen et al 2016 paper and STAN for linear mixed models at https://people.bath.ac.uk/jjf23/stan/

lower priority -> write code for nonlinear model
-- see stan_nlmer and vignette
-- see vignette("brms_nonlinear")
-- https://discourse.mc-stan.org/t/hierarchical-nonlinear-regression/4382/12
-- https://mattstats.wordpress.com/2017/06/12/fitting-nonlinear-functions-in-stan/

lower priority -> write nonlinear mixed effects model
--see links in above todo item

other lower priority issues/future extensions:

- consider alternate parameterization delta_1=log(gamma_1); delta_j=log(gamma_j-gamma_{j-1}) from Albert & Chib 1997 or just gamma_j-gamma_{j-1}

* repeated measures/longitudinal with PPO vs. NPO -- with a eye to applying these models for repeated measures ordinal data with censoring and a terminating event (e.g. ischemia QOL & death). The first steps for this part are determining what sort of monotonic censoring patterns/restrictions can be handled and how to correctly account for variance of estimates. Exploration of the relationship between this model formulation and a multistate model with censoring and an absorbing state could be relevant  <-- might need to move this to another paper??

* repeated ordinal w/ terminating event and censoring

* multiple detection limits within or across sites (hierarch on groups defined by detection limit)
-->


<!-- ## Background -->

Models for ordinal outcomes and extensions of these models using the Bayesian paradigm have been discussed extensively in the literature. 

<!-- mini lit review for ordinal models and summarizing results of previous continuous CPM papers -->

<!-- traditional models -->
McCullagh and Nelder (other early ref)for frequentist ordinal models.

<!-- Bayesian models -->
Under the Bayesian paradigm [name] describe the use of ... Albert and Chib [@albert_bayesian_1997], Johnson and Albert [@johnson_ordinal_1999], Congdon [@congdon_bayesian_2005], Others??

<!--freq continuous CPM -->
Harrell [@harrell_regression_2015] and Liu [@liu_modeling_2017] describe how non-parametric maximum likelihood (NPMLE) estimation can be used for continuous outcomes under the CPM modeling framework. A key insight for these models is that continuous data can also be viewed as ordinal categorical data. CPMs for continuous outcomes have several advanatages including direct modeling of the full conditional cumulative distribution function, invariance of regression coefficient estimates to monotonic outcome transformations, and ability to handle mixed continuous/discrete outcomes such as those that arise with a lower or upper limits of detection.

<!-- what did we do? --> 
In this paper, we develop Bayesian CPMs for continuous (and mixed?) outcomes. Bayesian CPMs inherit many of the properties of CPMs estimated using NPMLE and also have additional advantages including interpretation using posterior probbilities, andand exact inference (within simulation error) when the log-likelihood is not well-approximated by a quadratic function (ability to incorporate prior information if available, extensions to hierarchical, mixture models). Through simulations, we explore characteristics of these models and make comparisons to CPM estimates using NPMLE and Bayesian nonparametric (BNP) regression, another popular approach for flexibly modeling outcomes.
Finally, we present a case study involving HIV biomarker data which involves skewed outcomes censored at a lower limit of detection.

# Methods

## Cumulative Probability Models

<!--The model benefits, including Bayesian benefits-->

Cumulative Probability Models (CPM) are typically used for ordinal categorical outcomes. However, continuous outcomes are also ordinal. Using CPM models has several advantages including invariance to monotonic transformations and modeling the full conditional CDF which also allows estimates of means and quantiles to be calculated from a single model. CPMs can also handle an ordered mix of discrete/continuous outcome values, e.g. lower limit of detection <!--(Bayes extension hierarchical model/data augmentation for multiple censoring limits, etc?)-->. Utilizing a Bayesian paradigm allows generalization of the link function specification and exact inference for parameters and functionals based on posterior probabilities. For nonlinear models whose parameters are non-symmetric or multimodal, significant insight can be gained by examining the complete posterior distribution vs. point estimates and intervals from a non-Bayesian analysis.

### Model formulation
<!-- see albert & Chib 1997 - sec 2, and Johnson & Albert Ordinal Data Modeling to clean up this section-->
A Bayesian CPM for ordinal regression was described by Albert and Chib [@albert_bayesian_1993; @albert_bayesian_1997] and Johnson and Albert [@johnson_ordinal_1999]. Let $y_i$ ($i=1,\ldots,n$) be observed outcomes which each fall into one of $J$ ordered categories $1,\ldots, J$ and with $p$ associated covariates $\boldsymbol{x_i}=(x_{i1},\ldots,x_{ip})$. Suppose $Z_i=\boldsymbol{x_i'\beta}  + \epsilon_i$ are continuous latent variables underlying each $y_i$ with error distribution $\epsilon_i \sim F$ where $F(\cdot)$ is a specified cumulative distribution function and $\boldsymbol{\beta}$ is a $p \times 1$ vector of coefficients. Thus, the probability of observation $y_i$ falling into category $j$ given the covariates $x_i$ is $\pi_{ij}=Pr(y_i=j)=Pr(\gamma_{j-1} < Z_i < \gamma_j)=F(\gamma_{j} - x_i'\beta) - F(\gamma_{j-1} - x_i'\beta)$. The $\gamma_j$ ($j=1,\ldots, J-1$) are ordered cutpoints which categorize the outcome and we assume $\gamma_0=-\infty$ and $\gamma_J=\infty$ (index from 0 or 1??). Note that the value of the $\pi_{ij}$ are dependent on $x_i$, but we suppress the conditional notation except where it is needed for clarity. The cumulative probability of falling into category $j$ or a lower category is $\eta_{ij}=\sum_{k=1}^{j}\pi_{ik}$ and the cumulative probability model is
\begin{gather}
Pr(Y \le j|X,\beta,\gamma)= \eta_{ij} = F(\gamma_{j}-\boldsymbol{x_i'\beta}),\, \text{for }i=1,\ldots,n,\, j=1,\ldots,J-1 
\end{gather}

As noted by Johnson & Albert [@johnson_ordinal_1999], the model with $J$ categories and $J-1$ cutpoints is nonidentifiable. There are two approaches to resolve this issue. The first approach fixes the value of one cutpoint or equivalently fixes the intercept in the regression function $Z=\boldsymbol{x'\beta} + \epsilon$ to be constant. Applying noninformative priors for the remaining cutpoints and $\beta$s along with the constraint $\gamma_1 \le \cdots \le \gamma_{J-1}$ results in maximum a posteriori (MAP) estimates which are equivalent to MLE estimates if there are moderate counts observed in each category. 

A second approach defines a probability distribution on the vector of cutpoints. Rather than specifying a prior directly on the $\gamma_i$, it is more intuitive to specify priors on the probability of falling into category $j$ for a fixed $\boldsymbol{x_i}$ and then use the specified distribution function to transform to the correct scale. If the observed $\boldsymbol{y}=(y_1, \ldots, y_n)$ are assumed independent given $\boldsymbol{\pi_i}=(\pi_{i1},\ldots,\pi_{iJ})$ the likelihood of $\boldsymbol{y}$ is proportional to a multinomial density $p(\boldsymbol{y}|\boldsymbol{\pi_i}) \propto \prod_{i=1}^{n} \pi_{ij}$. Using the prior $p(\boldsymbol{\gamma},\boldsymbol{\beta})$, the joint posterior distribution for ($\boldsymbol{\gamma},\boldsymbol{\beta})$ is
\begin{gather}
p(\boldsymbol{\gamma},\boldsymbol{\beta}|\boldsymbol{x},\boldsymbol{y}) \propto
p(\boldsymbol{\gamma},\boldsymbol{\beta})\prod_{i=1}^{n} \pi_{ij}=
p(\boldsymbol{\gamma},\boldsymbol{\beta})\prod_{i=1}^{n}[F(\gamma_{y_i}-\boldsymbol{x_i}'\beta )-F(\gamma_{y_i-1}-\boldsymbol{x_i}'\beta)]
\end{gather} 
!!! in above need summation/prod over $j$??, or indicator that for obs $i$ it is in category $j$ !!!

Because $\boldsymbol{y}$ is multinomial a natural choice for the prior distribution is a conjugate Dirichlet prior $p(\boldsymbol{p_i})\propto \prod_{i=1}^{n}p_i^{\alpha_i-1}$ where $\alpha_i>0$ are $n$ concentration parameters. !!!clarify that the prior is on the p_{ij} conditional on x (I think x=0), check this !!! Using this prior, the concentration parameters can be interpreted as prior 'pseudo-observations' in the each of the $J$ categories and the posterior is also Dirichlet. For example, using a symmetric Dirichlet distribution so $\alpha_1=\alpha_2=\cdots=\alpha_n=1$ implies a posterior distribution of $p(\boldsymbol{p_i}|y) \propto Dirichlet(1+n_1,1+n_2,\ldots,1+n_J)$ where $n_j$ is the number of observations in each category.

<!--
$\alpha=1$ equally favor all vectors $p_i$ such that $\sum p_i =1$
$\alpha < 1$ favors sparsity in most cells with high prob for small number of cells
$\alpha > 1$ favors equally distributed counts  (If all concentration parameters are equal but greater than 1 then the prior mode is that the categories are equiprobable, and the larger the value of the identical concentration parameters, the more sharply peaked the distribution is at the mode. )

plot to demonstrate this??
-->

<!--
let $\pi_j=P(y=j|\bar{x})=?? p_j ??$ such that $\sum_{j=1}^{J}\pi_j=1$ and note that $(\pi_1,\ldots,\pi_{j-1})$ follows a Dirichlet distribution

$F^{-1}(\eta_{ij})=\gamma_j-x_i'\beta$
$\gamma_j=F^{-1}(\sum_{k=1}^{j} p_{ik})+x_i'\beta$
$\gamma_j=F\left(\sum_{k=1}^{j} p_{ik} \right)$. 
-->

Using the relationship between the probability of falling into each category and the link function, the prior for the cutpoints is then $p(\boldsymbol{\gamma}) = p(h(\boldsymbol{\gamma,\beta}))|\mathcal{J}|$ where $h(\boldsymbol{\gamma,\beta})=F(\gamma - x'\beta)$ is the transformation from the cutpoints $\boldsymbol{\gamma}$ to the probabilities $\boldsymbol{p_i}$ and $\mathcal{J}$ is the Jacobian of the transformation.

```{r}
# ordinal regression model with 10 categories and symmetric Dirichlet with alpha=1

# prior predictive distribution

# posterior distribution
```

Priors for $\beta$ coefficients (flat, weakly informative, other?)

Noninformative or weakly informative priors can be used for the $\beta$ coefficients in the absence of prior information.


### CPM for continuous outcomes

In the usual ordinal CPM described above, many of the $j=1,\ldots,J$ categories contain multiple observations, however for continuous data with no ties each category has only one observation. 

[Heuristic argument]
For continuous outcome $y$ with no ties, the dimension of the $\gamma$ parameter grows at the same rate as the sample size $n$, so an uniformative prior for $p(\boldsymbol{\gamma})$ will be highly influential on posterior inference.

assume symmetric Dirichlet distribution $\alpha_1 = \alpha_2 = \cdots = \alpha_n$

- If use improper $\alpha=0$, revert to previous noninformative case and nonidentifiable unless additional constraint is added
- Using $\alpha=1$ which equally favors all vectors $p_i$ such that $\sum p_i=1$ adds as many observations from the prior as are observed (each cell has count 2, one from prior & one from observed data)


Rather than specifying concentration, estimate $\alpha$ distribution with $\alpha \sim Gamma(a, b)$.  !!!May need to use different distribution to make sure prior only adds a little info while still regularizing enough to estimate intercepts !!! <!--(plot draws from this?)--> Still might have issues since implies all $\alpha_k$ are centered around same location, solve this problem by using dirichlet process mixture 


(<--need to bolster this argument!!, check lit, BDA3 etc.) (when all other params fixed, what does likelihood of say, $\gamma_2$ look like? show why uninformative prior won't work)

Hyp. For a low to moderate number of ordinal categories, both fixed conc and estimating conc perform similarly. [Add sentences/evidence to back this up. Maybe diagnostics/model fit (posterior predictive checks)]

Hyp. However, when the number of ordinal categories increases and the number of events per category is low, the fixed concentration parameter model (fails) to accurate recover the underlying [intercepts? distribution?].  

Hyp. there is some sort of shrinkage or regularization going on that is too strong with fixed parameter.


[Speculative statement: This implies that information supplied by the Dirichlet prior has higher weight since each cell is a combination of 'prior count' and a single observation - need to show this]. (Is Dir. with $\alpha=1$ the equivalent of noninformative prior for multinomial?? - see BDA3 chap on Bayesian histogram)

[General question, model is motivated by positing latent variables, e.g. conditional on covars; why not actually incorporate these latent variables using some sort of data augmentation or EM algorithm approach?]

change in likelihood for continuous?? 
\begin{gather*}
p(\boldsymbol{\gamma},\boldsymbol{\beta}|\boldsymbol{X},\boldsymbol{Y}) \propto p(\boldsymbol{\gamma},\boldsymbol{\beta})\prod_{i=1}^{n}[F(y_i|x_i)-F(y_i - |x_i)]
\end{gather*} 

where $F(y -|x)=\lim_{t \uparrow y}F(t|x)$


from Albert & Chib 1997; what is lumping property of Dirichlet??

(what about different type of uninformative prior, e.g. Jeffrey's prior?? or Albert 1997 prior on unconstrained intercepts?? - Maybe just mention that other parameterizations for intercepts have been explored)



By default, $\alpha_i=1\, \forall i$ (i.e., prior count of 1 in each bin)
allow $\gamma \sim ??$ (ie hyperprior on concentration param)

add hyperprior so $\alpha \sim Gamma(a,b)$ (some sort of weakly informative, positive distribution such as centered around 1)



```{r}
library(gtools)

# from Wikipedia
# Values of the concentration parameter above 1 prefer variates that are dense, evenly distributed distributions, i.e. all the values within a single sample are similar to each other. Values of the concentration parameter below 1 prefer sparse distributions, i.e. most of the values within a single sample will be close to 0, and the vast majority of the mass will be concentrated in a few of the values. 

# alpha (conc) = 1, uniform over discrete dists with 4 categories
d1<-rdirichlet(20, c(1,1,1,1))

# alpha (conc) = 15, discrete dists with 4 balanced category probabilities more likely
d2<-rdirichlet(20, c(15,15,15,15))

# alpha (conc) = 0.5, discrete dists with 4 unbalanced category probabilities more likely
d3<-rdirichlet(20, c(0.5,0.5,0.5,0.5))

```

### Estimation of functionals (Mean, Quantiles, etc)

[description of how to use Bayesian CPM to get parameter ests. for shifts in distribution and other quantities of interest, e.g. Mean, Quantiles, Exceedance probabilities using a single model]

<!--
can we just use, e.g. posterior predictive values to get these with normal Bayesian regression model?? - maybe not because likelihood/sampling model may not be flexible enough
-->

## Nonparametric Model

[comparison to other Bayesian nonparametric approach, e.g. Dirichlet process or Polya trees <!-- - what are advantages of this model? - computation efficiency?? other? !! need to research these more --> ]

Note: want to use BNP model that allows flexibility in the residual distribution first
$y_i = f(x_i) + \epsilon_i$ with $\epsilon_i|G \stackrel{iid}\sim G$ with BNP for $G$ See Muller Chp 4, esp 4.2 and Chp 3. For later models might want flexibility in $f(x_i)$ or completely nonparametric (i.e. density regression)

<!--
Dirichlet process model for epsilon in Y=x'beta + epsilon

describe model

show connection to CPM (limiting case?)

empirical Bayesian histogram (e.g., Dirichlet process)
-->

## Extending the CPM model

<!---
Not sure if mixture link is needed to fit model well. Evidence from previous papers and CPM poster show ok robustness, except for extreme link misspecification
-->

* Mixture link

One important component of the CPM is the specification of the link function $F(\cdot)$. Although the CPM model estimated using NPMLE is somewhat robust to link function misspecification it is of interest to determine the degree to which the data support one link function over another. In the Bayesian model, we can model a mixture of several link functions while incorporating the additional uncertainty inherent in allowing a combination of multiple links.

Consider both finite mixture $F_{mix}=w_1F_1 + w_2F_2 + (1-w_1-w_2)F_3$ and BNP prior over normal distributions to get nonparametric estimate of link 

[need to research more about mixture models]
[add Stan model implementing mixture link]
Let $F_1 \sim loglog$, $F_2 \sim logistic$, $F_3 \sim cloglog$ be three possible choices for link function. Then $F_{mix}=w_1F_1 + w_2F_2 + (1-w_1-w_2)F_3$ is a mixture. The priors for weights $w_1$, $w_2$, $w_3$ such that $\sum_i w_i = 1$

Incorporating this extension the full model becomes

$p(\boldsymbol{\gamma},\boldsymbol{\beta},\boldsymbol{w}|\boldsymbol{X},\boldsymbol{Y}) \propto p(\boldsymbol{\gamma},\boldsymbol{\beta}, \boldsymbol{w})\prod_{i=1}^{n}[F_{mix}(y_i|x_i)-F_{mix}(y_i - |x_i)]$

(do we need to get inverse?, i.e. $F_{mix}^{-1}(\cdot)$)

also consider using some sort of DP prior over normal distributions to get nonparametric estimate of link (lose interpretability and computational efficiency, but might have more robustness). One or the other might be better depending on the sample size, complexity of outcome, etc 

<!-- don't include this for now
* PO, NPO, PPO extension
use selection for PO/NPO by variable -> see McKinley et al. [@mckinley_bayesian_2015]
-->
 
* Nonlinear model extension - 
nonlinear in variables (splines etc) and nonlinear in parameters

* Longitudinal model extension - 
nonlinear mixed effects (will eventually be used for PK/PD modeling)


## Simulation Set-up

models for comparison:  

1) Bayes CPM continuous extension only  
<!--- 2) Bayes CPM continuous extension with mixture link  -->
2) BNP model
--> consider diff outcome scenarios, link misspecification



To evaluate the performance of our Bayesian CPM we use the following simulation set-up:

\begin{itemize}
\item Data for $n=50$ observations generated from $Y =\beta X+\varepsilon$ with $\beta=3$, $X \sim Bernoulli(0.5)$, and $\varepsilon \sim Logistic(0,1)$\\ 
\item 10,000 posterior MCMC draws produced using CPM with logit link [$G(p)=\log(\frac{p}{1-p})$]
\end{itemize}

weird shapes (i.e. mixture distributions); mixture continuous/discrete (lower limit of detection) - see Yuqi's paper for ideas


Also need comparisons/sims for extensions:
Bayes CPM nonlinear  
Bayes CPM mixed effects  

nonlinear - use Emax model


[section describing how models were actually fit, i.e. used HMC implemented in Stan, JAGS, dirichletprocess etc]


## Case Study

Use HIV data from Dr. Koethe. Describe background, data and modeling approach


# Results


## Simulations


Fig __ presents the results of the Bayesian CPM model using hierarchical hyperpriors for the Dirichlet prior concentration parameter compared to [a fixed concentration parameter $\alpha=1$ which corresponds to a uniform distribution over the distribution of cell probabilities - e.g. default in Stan (and recommendation in Albert & Chib??)] 

The conditional CDF \@ref(fig:fig1) and posterior mean and 75th percentile \@ref(fig:fig2).
```{r fig1, fig.cap="Conditional CDF", fig.align='center', out.width='65%'}
knitr::include_graphics(file.path(wd,"fig","cdf1.pdf"))
```

```{r fig2, fig.cap="Posterior Mean and 75th percentile", fig.align='center', out.width='75%'}
knitr::include_graphics(file.path(wd,"fig","mean_qtile1.pdf"))
```

## Case Study 


# Discussion

